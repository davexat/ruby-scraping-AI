Titulo,Link,Contenido Completo
OpenAI’s child exploitation reports increased sharply this year,https://arstechnica.com/tech-policy/2025/12/openais-child-exploitation-reports-increased-sharply-this-year/,"OpenAI sent 80 times as many child exploitation incident reports to the National Center for Missing & Exploited Children during the first half of 2025 as it did during a similar time period in 2024, according to a recent update from the company. The NCMEC’s CyberTipline is a Congressionally authorized clearinghouse for reporting child sexual abuse material (CSAM) and other forms of child exploitation.

Companies are required by law to report apparent child exploitation to the CyberTipline. When a company sends a report, NCMEC reviews it and then forwards it to the appropriate law enforcement agency for investigation.

Statistics related to NCMEC reports can be nuanced. Increased reports can sometimes indicate changes in a platform’s automated moderation, or the criteria it uses to decide whether a report is necessary, rather than necessarily indicating an increase in nefarious activity.

Additionally, the same piece of content can be the subject of multiple reports, and a single report can be about multiple pieces of content. Some platforms, including OpenAI, disclose the number of both the reports and the total pieces of content they were about for a more complete picture.

OpenAI spokesperson Gaby Raila said in a statement that the company made investments toward the end of 2024 “to increase [its] capacity to review and action reports in order to keep pace with current and future user growth.” Raila also said that the time frame corresponds to “the introduction of more product surfaces that allowed image uploads and the growing popularity of our products, which contributed to the increase in reports.” In August, Nick Turley, vice president and head of ChatGPT, announced that the app had four times the number of weekly active users than it did the year before.

During the first half of 2025, the number of CyberTipline reports OpenAI sent was roughly the same as the amount of content OpenAI sent the reports about—75,027 compared to 74,559. In the first half of 2024, it sent 947 CyberTipline reports about 3,252 pieces of content. Both the number of reports and pieces of content the reports saw a marked increase between the two time periods.

Content, in this context, could mean multiple things. OpenAI has said that it reports all instances of CSAM, including uploads and requests, to NCMEC. Besides its ChatGPT app, which allows users to upload files—including images—and can generate text and images in response, OpenAI also offers access to its models via API access. The most recent NCMEC count wouldn’t include any reports related to video-generation app Sora, as its September release was after the time frame covered by the update.

The spike in reports follows a similar pattern to what NCMEC has observed at the CyberTipline more broadly with the rise of generative AI. The center’s analysis of all CyberTipline data found that reports involving generative AI saw a 1,325 percent increase between 2023 and 2024. NCMEC has not yet released 2025 data, and while other large AI labs like Google publish statistics about the NCMEC reports they’ve made, they don’t specify what percentage of those reports are AI-related.

OpenAI’s update comes at the end of a year where the company and its competitors have faced increased scrutiny over child safety issues beyond just CSAM. Over the summer, 44 state attorneys general sent a joint letter to multiple AI companies including OpenAI, Meta, Character.AI, and Google, warning that they would “use every facet of our authority to protect children from exploitation by predatory artificial intelligence products.” Both OpenAI and Character.AI have faced multiple lawsuits from families or on behalf of individuals who allege that the chatbots contributed to their children’s deaths. In the fall, the US Senate Committee on the Judiciary held a hearing on the harms of AI chatbots, and the US Federal Trade Commission launched a market study on AI companion bots that included questions about how companies are mitigating negative impacts, particularly to children. (I was previously employed by the FTC and was assigned to work on the market study prior to leaving the agency.)

In recent months, OpenAI has rolled out new safety-focused tools more broadly. In September, OpenAI rolled out several new features for ChatGPT, including parental controls, as part of its work “to give families tools to support their teens’ use of AI.” Parents and their teens can link their accounts, and parents can change their teen’s settings, including by turning off voice mode and memory, removing the ability for ChatGPT to generate images, and opting their kid out of model training. OpenAI said it could also notify parents if their teen’s conversations showed signs of self-harm, and potentially also notify law enforcement if it detected an imminent threat to life and wasn’t able to get in touch with a parent.

In late October, to cap off negotiations with the California Department of Justice over its proposed recapitalizations plan, OpenAI agreed to “continue to undertake measures to mitigate risks to teens and others in connection with the development and deployment of AI and of AGI.” The following month, OpenAI released its Teen Safety Blueprint, in which it said it was constantly improving its ability to detect child sexual abuse and exploitation material, and reporting confirmed CSAM to relevant authorities, including NCMEC.

This story originally appeared at WIRED.com"
World’s largest shadow library made a 300TB copy of Spotify’s most streamed songs,https://arstechnica.com/tech-policy/2025/12/worlds-largest-shadow-library-brags-it-scraped-300tb-of-spotify-music-metadata/,"The world’s largest shadow library—which is increasingly funded by AI developers—shocked the Internet this weekend by announcing it had “backed up Spotify” and started distributing 300 terabytes of metadata and music files in bulk torrents.

According to Anna’s Archive, the data grab represents more than 99 percent of listens on Spotify, making it “the largest publicly available music metadata database with 256 million tracks.” It’s also “the world’s first ‘preservation archive’ for music which is fully open,” with 86 million music files, the archive boasted.

The music files supposedly represent about 37 percent of songs available on Spotify as of July 2025. The scraped files were prioritized by popularity, with Anna’s Archive weeding out many songs that are never streamed or are of poor quality, such as AI-generated songs.

On Monday, Spotify told Android Authority on Monday that it was investigating whether Anna’s Archive had actually scraped its platform “at scale,” as its blog claimed.

“An investigation into unauthorized access identified that a third party scraped public metadata and used illicit tactics to circumvent DRM to access some of the platform’s audio files,” Spotify said. “We are actively investigating the incident.”

It’s unclear how much Spotify data was actually scraped, Android Authority noted, or if the company will possibly pursue legal action to take down the torrents. Asked for comment, a Spotify spokesperson told Ars that “Spotify has identified and disabled the nefarious user accounts that engaged in unlawful scraping.”

For Anna’s Archive, the temptation to scrape the data may have been too much after stumbling upon “a way to scrape Spotify at scale,” supposedly “a while ago.”

“We saw a role for us here to build a music archive primarily aimed at preservation,” the archive said. Scraping Spotify data was a “great start,” they said, toward building an “authoritative list of torrents aiming to represent all music ever produced.”

A list like that “does not exist for music,” the archive said, and would be akin to LibGen—which was used by tech giants like Meta and startups like Anthropic to notoriously pirate book datasets to train AI.

Releasing the metadata torrents this December was the first step toward achieving this “preservation” mission, Anna’s Archive said. Next, the Archive will release torrents of music files, starting with the most popular streams first, then eventually releasing torrents of less popular songs and album art. In the future, “if there is enough interest, we could add downloading of individual files to Anna’s Archive,” the blog said.

Spotify told Ars that it’s taking steps to avoid any future scraping.

“We’ve implemented new safeguards for these types of anti-copyright attacks and are actively monitoring for suspicious behavior,” Spotify’s spokesperson said. “Since day one, we have stood with the artist community against piracy, and we are actively working with our industry partners to protect creators and defend their rights.”

Anna’s Archive claimed that the Spotify data was scraped to help preserve “humanity’s musical heritage,” protecting it “forever” from “destruction by natural disasters, wars, budget cuts, and other catastrophes.”

However, some Anna’s Archive fans—who largely use the search engine to find books, academic papers, and magazine articles—were freaked out by the news that Spotify data was scraped. On Hacker News, some users questioned whether the data would be useful to anyone but AI researchers, since searching bulk torrents for individual songs seemed impractical for music fans.

One user pointed out that “there are already tools to automatically locate and stream pirated TV and movie content automatic and on demand""—suggesting that music fans could find a way to stream the data. But others worried Anna’s Archive may have been baited into scraping Spotify, perhaps taking on legal risks that AI companies prone to obscuring their training data sources likely wish to avoid.

“This is insane,” a top commenter wrote. “Definitely wondering if this was in response to desire from AI researchers/companies who wanted this stuff. Or if the major record labels already license their entire catalogs for training purposes cheaply enough, so this really is just solely intended as a preservation effort?”

But Anna’s Archive is clearly working to support AI developers, another noted, pointing out that Anna’s Archive promotes selling “high-speed access” to “enterprise-level” LLM data, including “unreleased collections.” Anyone can donate “tens of thousands” to get such access, the archive suggests on its webpage, and any interested AI researchers can reach out to discuss “how we can work together.”

“AI may not be their original/primary motivation, but they are evidently on board with facilitating AI labs piracy-maxxing,” a third commenter suggested.

Meanwhile, on Reddit, some fretted that Anna’s Archive may have doomed itself by scraping the data. To them, it seemed like the archive was “only making themselves a target” after watching the Internet Archive struggle to survive a legal attack from record labels that ended in a confidential settlement last year.

“I’m furious with AA for sticking this target on their own backs,” a redditor wrote on a post declaring that “this Spotify hacking will just ruin the actual important literary archive.”

As Anna’s Archive fans spiraled, a conspiracy was even raised that the archive was only “doing it for the AI bros, who are the ones paying the bills behind the scenes” to keep the archive afloat.

Ars could not immediately reach Anna’s Archive to comment on users’ fears or Spotify’s investigation.

On Reddit, one user took comfort in the fact that the archive is “designed to be resistant to being taken out,” perhaps preventing legal action from ever really dooming the archive.

“The domain and such can be gone, sure, but the core software and its data can be resurfaced again and again,” the user explained.

But not everyone was convinced that Anna’s Archive could survive brazenly torrenting so much Spotify data.

“This is like saying the Titanic is unsinkable” that user warned, suggesting that Anna’s Archive might lose donations if Spotify-fueled takedowns continually frustrate downloads over time. “Sure, in theory data can certainly resurface again and again, but doing so each time, it will take money and resources, which are finite. How many times are folks willing to do this before they just give up?”

This story was updated to include Spotify’s statement. "
Google lobs lawsuit at search result scraping firm SerpApi,https://arstechnica.com/google/2025/12/google-lobs-lawsuit-at-search-result-scraping-firm-serpapi/,"Google has filed a lawsuit to protect its search results, targeting a firm called SerpApi that has turned Google’s 10 blue links into a business. According to Google, SerpApi ignores established law and Google’s terms to scrape and resell its search engine results pages (SERPs). This is not the first action against SerpApi, but Google’s decision to go after a scraper could signal a new, more aggressive stance on protecting its search data.

SerpApi and similar firms do fulfill a need, but they sit in a legal gray area. Google does not provide an API for its search results, which are based on the world’s largest and most comprehensive web index. That makes Google’s SERPs especially valuable in the age of AI. A chatbot can’t summarize web links if it can’t find them, which has led companies like Perplexity to pay for SerpApi’s second-hand Google data. That prompted Reddit to file a lawsuit against SerpApi and Perplexity for grabbing its data from Google results.

Google is echoing many of the things Reddit said when it publicized its lawsuit earlier this year. The search giant claims it’s not just doing this to protect itself—it’s also about protecting the websites it indexes. In Google’s blog post on the legal action, it says SerpApi “violates the choices of websites and rightsholders about who should have access to their content.”

It’s worth noting that Google has a partnership with Reddit that pipes data directly into Gemini. As a result, you’ll often see Reddit pages cited in the chatbot’s outputs. As Google points out, it abides by “industry-standard crawling protocols” to collect the data that appears on its SERPs, but those sites didn’t agree to let SerpApi scrape their data from Google. So while you could reasonably argue that Google’s lawsuit helps protect the rights of web publishers, it also explicitly protects Google’s business interests.

The kind of data scraping that SerpApi uses is not new—Google has simply put up with it until now. However, the company claims that SerpApi’s deceptive behavior, like spoofing user agents and hammering sites with armies of bots, has increased significantly over the past year. That may have something to do with the ever-growing hunger for search data in the AI era.

Google is coming out of its antitrust cases relatively unscathed so far, which affects the availability of its data. It has avoided the harshest remedies, like the government’s demand that Google offer search data to competitors. That might have turned businesses like SerpApi legit, but with that no longer an option, Google may feel emboldened to unleash the lawyers.

While Google paints this as a benevolent action, it stands to benefit by stopping SERP scrapers. However, this may make some of Google’s competitors happy, too. Since the search giant doesn’t offer an official API for search (and the courts declined to force the issue), SERP scraping provides a way for companies to get Google’s search data on the down-low. This reduces demand for the handful of other web indexes that do have APIs, including Brave and Bing. Without illicit search APIs, chatbots may have to lean on official data sources."
LG TVs’ unremovable Copilot shortcut is the least of smart TVs’ AI problems,https://arstechnica.com/gadgets/2025/12/lg-tvs-unremovable-copilot-shortcut-is-the-least-of-smart-tvs-ai-problems/,"Online fury erupted this week after an LG TV owner claimed that a firmware update installed unremovable generative AI software on their smart TV.

The controversy began on Saturday, when a Reddit user posted about the sudden appearance of a Microsoft Copilot icon on their device (something Windows users are all too familiar with). The Reddit user claimed that a “new software update installed Copilot” onto their LG TV and that it couldn’t be deleted.

“Pre-installed crap is universally dogshit. If I wanted it, I’d have installed it myself eventually. The whole reason it’s bundled is because no one would choose it… Burn your television,” another Reddit user responded in the thread, which has 36,000 upvotes as of this writing.

Some news outlets covered the move this week, reporting that LG customers were upset by an “unremovable Microsoft Copilot installation” and that other LG TV owners could expect to get it the next time they update their device.

LG has since admitted that it used a webOS update to force Copilot onto some of its TVs. However, the firmware update didn’t install the Copilot application but rather a shortcut to the Copilot web app, which opens in the TV’s integrated web browser, LG spokesperson Chris De Maria told The Verge. De Maria added that “features such as microphone input are activated only with the customer’s explicit consent.”

LG’s spokesperson said the company added the “shortcut icon to enhance customer accessibility and convenience.” LG, like other companies, has failed to prove why people need their non-computing devices, such as computer mice or earbuds, to provide instant access to third-party chatbots.

For those annoyed about the web app shortcut, De Maria said today that LG “will take steps to allow users to delete the shortcut icon if they wish.” He didn’t provide more details.

Even with LG’s concession, it may become more difficult to avoid chatbots on TVs.

LG says it will let people delete the Copilot icon from their TVs soon, but it still has plans to weave the service throughout webOS. The Copilot web app rollout seems to have been a taste of LG’s bigger plans to add Copilot to some of its 2025 OLED TVs. In a January announcement, LG said Copilot will help users find stuff to watch by “allowing users to efficiently find and organize complex information using contextual cues.” LG also said Copilot would “proactively” identify potential user problems and offer “timely, effective solutions.”

Some TVs from LG’s biggest rival, Samsung, have included Copilot since August. Owners of supporting 2025 TVs can speak to Copilot using their remote’s microphone. They can also access Copilot via the Tizen OS homescreen’s Apps tab or through the TVs’ Click to Search feature, which lets users press a dedicated remote button to search for content while watching live TV or Samsung TV Plus. Users can also ask the TV to make AI-generated wallpapers or provide real-time subtitle translations.

Copilot similarly rolled out automatically onto supporting Samsung TVs. Users can avoid Copilot by not using the above features, and they may be able to remove the Copilot icon from their TV’s Apps section.

But Copilot will still be integrated into Tizen OS, and Samsung appears eager to push chatbots into TVs, including by launching Perplexity’s first TV app. Amazon, which released Fire TVs with Alexa+ this year, is also exploring putting chatbots into TVs.

After the backlash LG faced this week, companies may reconsider installing AI apps on people’s smart TVs. A better use of large language models in TVs may be as behind-the-scenes tools to improve TV watching. People generally don’t buy smart TVs to make it easier to access chatbots.

But this development is still troubling for anyone who doesn’t want an AI chatbot in their TV at all.

Subtle integrations of generative AI that make it easier for people to do things like figure out the name of “that movie” may have practical use, but there are reasons to be wary of chatbot-wielding TVs.

Chatbots add another layer of complexity to understanding how a TV tracks user activity. With a chatbot involved, smart TV owners will be subject to complicated smart TV privacy policies and terms of service, as well as the similarly verbose rules of third-party AI companies. This will make it harder for people to understand what data they’re sharing with companies, and there’s already serious concern about the boundaries smart TVs are pushing to track users, including without consent.

Chatbots can also contribute to smart TV bloatware. Unwanted fluff, like games, shopping shortcuts, and flashy ads, already disrupts people who just want to watch TV.

LG’s Copilot web app is worthy of some grousing, but not necessarily because of the icon that users will eventually be able to delete. The more pressing issue is the TV industry’s shift toward monetizing software with user tracking and ads.

If you haven’t already, now is a good time to check out our guide to breaking free from smart TV ads and tracking."
We asked four AI coding agents to rebuild Minesweeper—the results were explosive,https://arstechnica.com/ai/2025/12/the-ars-technica-ai-coding-agent-test-minesweeper-edition/,"The idea of using AI to help with computer programming has become a contentious issue. On the one hand, coding agents can make horrific mistakes that require a lot of inefficient human oversight to fix, leading many developers to lose trust in the concept altogether. On the other hand, some coders insist that AI coding agents can be powerful tools and that frontier models are quickly getting better at coding in ways that overcome some of the common problems of the past.

To see how effective these modern AI coding tools are becoming, we decided to test four major models with a simple task: re-creating the classic Windows game Minesweeper. Since it’s relatively easy for pattern-matching systems like LLMs to play off of existing code to re-create famous games, we added in one novelty curveball as well.

Our straightforward prompt:

Make a full-featured web version of Minesweeper with sound effects that

1) Replicates the standard Windows game and
2) implements a surprise, fun gameplay feature.

Include mobile touchscreen support.

Ars Senior AI Editor Benj Edwards fed this task into four AI coding agents with terminal (command line) apps: OpenAI’s Codex based on GPT-5, Anthropic’s Claude Code with Opus 4.5, Google’s Gemini CLI, and Mistral Vibe. The agents then directly manipulated HTML and scripting files on a local machine, guided by a “supervising” AI model that interpreted the prompt and assigned coding tasks to parallel LLMs that can use software tools to execute the instructions. All AI plans were paid for privately with no special or privileged access given by the companies involved, and the companies were unaware of these tests taking place.

Ars Senior Gaming Editor (and Minesweeper expert) Kyle Orland then judged each example blind, without knowing which model generated which Minesweeper clone. Those somewhat subjective and non-rigorous results are below.

For this test, we used each AI model’s unmodified code in a “single shot” result to see how well these tools perform without any human debugging. In the real world, most sufficiently complex AI-generated code would go through at least some level of review and tweaking by a human software engineer who could spot problems and address inefficiencies.

We chose this test as a sort of simple middle ground for the current state of AI coding. Cloning Minesweeper isn’t a trivial task that can be done in just a handful of lines of code, but it’s also not an incredibly complex system that requires many interlocking moving parts.

Minesweeper is also a well-known game, with many versions documented across the Internet. That should give these AI agents plenty of raw material to work from and should be easier for us to evaluate than a completely novel program idea. At the same time, our open-ended request for a new “fun” feature helps demonstrate each agent’s penchant for unguided coding “creativity,” as well as their ability to create new features on top of an established game concept.

With all that throat-clearing out of the way, here’s our evaluation of the AI-generated Minesweeper clones, complete with links that you can use to play them yourselves.

Play it for yourself

Implementation

Right away, this version loses points for not implementing chording—the technique that advanced Minesweeper players use to quickly clear all the remaining spaces surrounding a number that already has sufficient flagged mines. Without this feature, this version feels more than a little clunky to play.

I’m also a bit perplexed by the inclusion of a “Custom” difficulty button that doesn’t seem to do anything. It’s like the model realized that customized board sizes were a thing in Minesweeper but couldn’t figure out how to implement this relatively basic feature.

The game works fine on mobile, but marking a square with a flag requires a tricky long-press on a tiny square that also triggers selector handles that are difficult to clear. So it’s not an ideal mobile interface.

Presentation

This was the only working version we tested that didn’t include sound effects. That’s fair, since the original Windows Minesweeper also didn’t include sound, but it’s still a notable relative omission since the prompt specifically asked for it.

The all-black “smiley face” button to start a game is a little off-putting, too, compared to the bright yellow version that’s familiar to both Minesweeper players and emoji users worldwide. And while that smiley face does start a new game when clicked, there’s also a superfluous “New Game” button taking up space for some reason.

“Fun” feature

The closest thing I found to a “fun” new feature here was the game adding a rainbow background pattern on the grid when I completed a game. While that does add a bit of whimsy to a successful game, I expected a little more.

Coding experience

Benj notes that he was pleasantly surprised by how well Mistral Vibe performed as an open-weight model despite lacking the big-money backing of the other contenders. It was relatively slow, however (third fastest out of four), and the result wasn’t great. Ultimately, its performance so far suggests that with more time and more training, a very capable AI coding agent may eventually emerge.

Overall rating: 4/10

This version got many of the basics right but left out chording and didn’t perform well on the small presentational and “fun” touches.

Play it for yourself

Implementation

Not only did this agent include the crucial “chording” feature, but it also included on-screen instructions for using it on both PC and mobile browsers. I was further impressed by the option to cycle through “?” marks when marking squares with flags, an esoteric feature I feel even most human Minesweeper cloners might miss.

On mobile, the option to hold your finger down on a square to mark a flag is a nice touch that makes this the most enjoyable handheld version we tested.

Presentation

The old-school emoticon smiley-face button is pretty endearing, especially when you blow up and get a red-tinted “X(“. I was less impressed by the playfield “graphics,” which use a simple “*” for revealed mines and an ugly red “F” for flagged tiles.

The beeps-and-boops sound effects reminded me of my first old-school, pre-Sound-Blaster PC from the late ’80s. That’s generally a good thing, but I still appreciated the game giving me the option to turn them off.

“Fun” feature

The “Surprise: Lucky Sweep Bonus” listed in the corner of the UI explains that clicking the button gives you a free safe tile when available. This can be pretty useful in situations where you’d otherwise be forced to guess between two tiles that are equally likely to be mines.

Overall, though, I found it a bit odd that the game gives you this bonus only after you find a large, cascading field of safe tiles with a single click. It mostly functions as a “win more” button rather than a feature that offers a good balance of risk versus reward.

Coding experience

OpenAI Codex has a nice terminal interface with features similar to Claude Code (local commands, permission management, and interesting animations showing progress), and it’s fairly pleasant to use (OpenAI also offers Codex through a web interface, but we did not use that for this evaluation). However, Codex took roughly twice as long to code a functional game than Claude Code did, which might contribute to the strong result here.

Overall: 9/10

The implementation of chording and cute presentation touches push this to the top of the list. We just wish the “fun” feature was a bit more fun.

Play it for yourself

Implementation

Once again, we get a version that gets all the gameplay basics right but is missing the crucial chording feature that makes truly efficient Minesweeper play possible. This is like playing Super Mario Bros. without the run button or Ocarina of Time without Z-targeting. In a word: unacceptable.

The “flag mode” toggle on the mobile version of this game is perfectly functional, but it’s a little clunky to use. It also visually cuts off a portion of the board at the larger game sizes.

Presentation

Presentation-wise, this is probably the most polished version we tested. From the use of cute emojis for the “face” button to nice-looking bomb and flag graphics and simple but effective sound effects, this looks more professional than the other versions we tested.

That said, there are some weird presentation issues. The “beginner” grid has weird gaps between columns, for instance. The borders of each square and the flag graphics can also become oddly grayed out at points, especially when using Power Mode (see below).

“Fun” feature

The prominent “Power Mode” button in the lower-right corner offers some pretty fun power-ups that alter the core Minesweeper formula in interesting ways. But the actual powers are a bit hit-and-miss.

I especially liked the “Shield” power, which protects you from an errant guess, and the “Blast” power, which seems to guarantee a large cascade of revealed tiles wherever you click. But the “X-Ray” power, which reveals every bomb for a few seconds, could be easily exploited by a quick player (or a crafty screenshot). And the “Freeze” power is rather boring, just stopping the clock for a few seconds and amounting to a bit of extra time.

Overall, the game hands out these new powers like candy, which makes even an Expert-level board relatively trivial with Power Mode active. Simply choosing “Power Mode” also seems to mark a few safe squares right after you start a game, making things even easier. So while these powers can be “fun,” they also don’t feel especially well-balanced.

Coding experience

Of the four tested models, Claude Code with Opus 4.5 featured the most pleasant terminal interface experience and the fastest overall coding experience (Claude Code can also use Sonnet 4.5, which is even faster, but the results aren’t quite as full-featured in our experience). While we didn’t precisely time each model, Opus 4.5 produced a working Minesweeper in under five minutes. Codex took at least twice as long, if not longer, while Mistral took roughly three or four times as long as Claude Code. Gemini, meanwhile, took hours of tinkering to get two non-working results.

Overall: 7/10

The lack of chording is a big omission, but the strong presentation and Power Mode options give this effort a passable final score.

Play it for yourself

Implementation, presentation, etc.

Gemini CLI did give us a few gray boxes you can click, but the playfields are missing. While interactive troubleshooting with the agent may have fixed the issue, as a “one-shot” test, the model completely failed.

Coding experience

Of the four coding agents we tested, Gemini CLI gave Benj the most trouble. After developing a plan, it was very, very slow at generating any usable code (about an hour per attempt). The model seemed to get hung up attempting to manually create WAV file sound effects and insisted on requiring React external libraries and a few other overcomplicated dependencies. The result simply did not work.

Benj actually bent the rules and gave Gemini a second chance, specifying that the game should use HTML5. When the model started writing code again, it also got hung up trying to make sound effects. Benj suggested using the WebAudio framework (which the other AI coding agents seemed to be able to use), but the result didn’t work, which you can see at the link above.

Unlike the other models tested, Gemini CLI apparently uses a hybrid system of three different LLMs for different tasks (Gemini 2.5 Flash Lite, 2.5 Flash, and 2.5 Pro were available at the level of the Google account Benj paid for). When you’ve completed your coding session and quit the CLI interface, it gives you a readout of which model did what.

In this case, it didn’t matter because the results didn’t work. But it’s worth noting that Gemini 3 coding models are available for other subscription plans that were not tested here. For that reason, this portion of the test could be considered “incomplete” for Google CLI.

Overall: 0/10 (Incomplete)

OpenAI Codex wins this one on points, in no small part because it was the only model to include chording as a gameplay option. But Claude Code also distinguished itself with strong presentational flourishes and quick generation time. Mistral Vibe was a significant step down, and Google CLI based on Gemini 2.5 was a complete failure on our one-shot test.

While experienced coders can definitely get better results via an interactive, back-and-forth code editing conversation with an agent, these results show how capable some of these models can be, even with a very short prompt on a relatively straightforward task. Still, we feel that our overall experience with coding agents on other projects (more on that in a future article) generally reinforces the idea that they currently function best as interactive tools that augment human skill rather than replace it."
YouTube bans two popular channels that created fake AI movie trailers,https://arstechnica.com/google/2025/12/youtube-bans-two-popular-channels-that-created-fake-ai-movie-trailers/,"Google is generally happy to see people using generative AI tools to create content, and it’s doubly happy when they publish it on its platforms. But there are limits to everything. Two YouTube channels that attracted millions of subscribers with AI-generated movie trailers have been shuttered.

Screen Culture and KH Studio flooded the site with fake but often believable trailers. The channels, which had a combined audience of more than 2 million subscribers, became a thorn in Google’s side in early 2025 when other YouTubers began griping about their sudden popularity in the age of AI. The channels produced videos with titles like “GTA: San Andreas (2025) Teaser Trailer” and “Malcom In The Middle Reboot (2025) First Trailer.” Of course, neither of those projects exist, but that didn’t stop them from appearing in user feeds.

Google demonetized the channels in early 2025, forcing them to adopt language that made it clear they were not official trailers. The channels were able to monetize again, but the disclaimers were not consistently used. Indeed, many of the most popular videos from those channels in recent months included no “parody” or “concept trailer” disclosures. Now, visiting either channel’s page on YouTube produces an error reading, “This page isn’t available. Sorry about that. Try searching for something else.”

Deadline reports that the behavior of these creators ran afoul of YouTube’s spam and misleading-metadata policies. At the same time, Google loves generative AI—YouTube has added more ways for creators to use generative AI, and the company says more gen AI tools are coming in the future. It’s quite a tightrope for Google to walk.

While passing off AI videos as authentic movie trailers is definitely spammy conduct, the recent changes to the legal landscape could be a factor, too. Disney recently entered into a partnership with OpenAI, bringing its massive library of characters to the company’s Sora AI video app. At the same time, Disney sent a cease-and-desist letter to Google demanding the removal of Disney content from Google AI. The letter specifically cited AI content on YouTube as a concern.

Both the banned trailer channels made heavy use of Disney properties, sometimes even incorporating snippets of real trailers. For example, Screen Culture created 23 AI trailers for The Fantastic Four: First Steps, some of which outranked the official trailer in searches. It’s unclear if either account used Google’s Veo models to create the trailers, but Google’s AI will recreate Disney characters without issue.

While Screen Culture and KH Studio were the largest purveyors of AI movie trailers, they are far from alone. There are others with five and six-digit subscriber counts, some of which include disclosures about fan-made content. Is that enough to save them from the ban hammer? Many YouTube viewers probably hope not."
School security AI flagged clarinet as a gun. Exec says it wasn’t an error.,https://arstechnica.com/tech-policy/2025/12/florida-schools-plan-to-vastly-expand-use-of-ai-that-mistook-clarinet-for-gun/,"A Florida middle school was locked down last week after an AI security system called ZeroEyes mistook a clarinet for a gun, reviving criticism that AI may not be worth the high price schools pay for peace of mind.

Human review of the AI-generated false flag did not stop police from rushing to Lawton Chiles Middle School. Cops expected to find “a man in the building, dressed in camouflage with a ‘suspected weapon pointed down the hallway, being held in the position of a shouldered rifle,’” a Washington Post review of the police report said.

Instead, after finding no evidence of a shooter, cops double-checked with dispatchers who confirmed that a closer look at the images indicated that “the suspected rifle might have been a band instrument.” Among panicked students hiding in the band room, police eventually found the suspect, a student “dressed as a military character from the Christmas movie Red One for the school’s Christmas-themed dress-up day,” the Post reported.

ZeroEyes cofounder Sam Alaimo told the Post that the AI performed exactly as it should have in this case, adopting a “better safe than sorry” outlook. A ZeroEyes spokesperson told Ars that “school resource officers, security directors and superintendents consistently ask us to be proactive and forward them an alert if there is any fraction of a doubt that the threat might be real.”

“We don’t think we made an error, nor does the school,” Alaimo said. “That was better to dispatch [police] than not dispatch.”

Cops left after the confused student confirmed he was “unaware” that the way he was holding his clarinet could have triggered that alert, the Post reported. But ZeroEyes’ spokesperson claimed he was “intentionally holding the instrument in the position of a shouldered rifle.” And seemingly rather than probe why the images weren’t more carefully reviewed to prevent a false alarm on campus, the school appeared to agree with ZeroEyes and blame the student.

“We did not make an error, and the school was pleased with the detection and their response,” ZeroEyes’ spokesperson said.

In a letter to parents, the principal, Melissa Laudani, reportedly told parents that “while there was no threat to campus, I’d like to ask you to speak with your student about the dangers of pretending to have a weapon on a school campus.” Along similar lines, Seminole County Public Schools (SCPS) communications officer, Katherine Crnkovich, emphasized in an email to Ars to “please make sure it is noted that this student wasn’t simply carrying a clarinet. This individual was holding it as if it were a weapon.”

However, warning students against brandishing ordinary objects like weapons isn’t a perfect solution. Video footage from a Texas high school in 2023 showed that ZeroEyes can sometimes confuse shadows for guns, accidentally flagging a student simply walking into school as a potential threat. The advice also ignores that ZeroEyes last year reportedly triggered a lockdown and police response after detecting two theater kids using prop guns to rehearse a play. And a similar AI tool called Omnilert made national headlines confusing an empty Doritos bag with a gun, which led to a 14-year-old Baltimore sophomore’s arrest. In that case, the student told the American Civil Liberties Union that he was just holding the chips when AI sent “like eight cop cars” to detain him.

For years, school safety experts have warned that AI tools like ZeroEyes take up substantial resources even though they are “unproven,” the Post reported. ZeroEyes’ spokesperson told Ars that “in most cases, ZeroEyes customers will never receive a ‘false positive,’” but the company is not transparent about how many false positives it receives or how many guns have been detected. An FAQ only notes that “we are always looking to minimize false positives and are constantly improving our learning models based on data collected.” In March, as some students began questioning ZeroEyes after it flagged a Nerf gun at a Pennsylvania university, a nearby K-12 private school, Germantown Academy, confirmed that its “system often makes ‘non-lethal’ detections.”

One critic, school safety consultant Kenneth Trump, suggested in October that these tools are “security theater,” with firms like ZeroEyes lobbying for taxpayer dollars by relying on what the ACLU called “misleading” marketing to convince schools that tools are proactive solutions to school shootings. Seemingly in response to this backlash, StateScoop reported that days after it began probing ZeroEyes in 2024, the company scrubbed a claim from its FAQ that said ZeroEyes “can prevent active shooter and mass shooting incidents.”

At Lawton Chiles Middle School, “the children were never in any danger,” police confirmed, but experts question if false positives cause students undue stress and suspicion, perhaps doing more harm than good in absence of efficacy studies. Schools may be better off dedicating resources to mental health services proven to benefit kids, some critics have suggested.

Laudani’s letter encouraged parents to submit any questions they have about the incident, but it’s hard to gauge if anyone’s upset. Asked if parents were concerned or if ZeroEyes has ever triggered lockdown at other SCPS schools, Crnkovich told Ars that SCPS does not “provide details regarding the specific school safety systems we utilize.”

It’s clear, however, that SCPS hopes to expand its use of ZeroEyes. In November, Florida state Senator Keith Truenow submitted a request to install “significantly more cameras""—about 850—equipped with ZeroEyes across the school district. Truenow backed up his request for $500,000 in funding over the next year by claiming that “the more [ZeroEyes] coverage there is, the more protected students will be from potential gun violence.”

ZeroEyes is among the most popular tools attracting heavy investments from schools in 48 states, which hope that AI gun detection will help prevent school shootings. The AI technology is embedded in security cameras, trained on images of people holding guns, and can supposedly “detect as little as an eighth of an inch of a gun,” an ABC affiliate in New York reported.

Monitoring these systems continually, humans review AI flags, then text any concerning images detected to school superintendents. Police are alerted when human review determines images may constitute actual threats. ZeroEyes’ spokesperson told Ars that “it has detected more than 1,000 weapons in the last three years.” Perhaps most notably, ZeroEyes “detected a minor armed with an AK-47 rifle on an elementary school campus in Texas,” where no shots were fired, StateScoop reported last year.

Schools invest tens or, as the SCPS case shows, even hundreds of thousands annually, the exact amount depending on the number of cameras they want to employ and other variables impacting pricing. ZeroEyes estimates that most schools pay $60 per camera monthly. Bigger contracts can discount costs. In Kansas, a statewide initiative equipping 25 cameras at 1,300 schools with ZeroEyes was reportedly estimated to cost $8.5 million annually. Doubling the number of cameras didn’t provide much savings, though, with ZeroEyes looking to charge $15.2 million annually to expand coverage.

To critics, it appears that ZeroEyes is attempting to corner the market on AI school security, standing to profit off schools’ fears of shootings, while showing little proof of the true value of its systems. Last year, ZeroEyes reported its revenue grew 300 percent year over year from 2023 to 2024, after assisting in “more than ten arrests through its thousands of detections, verifications, and notifications to end users and law enforcement.”

Curt Lavarello, the executive director of the School Safety Advocacy Council, told the ABC News affiliate that “all of this technology is very, very expensive,” considering that “a lot of products … may not necessarily do what they’re being sold to do.”

Another problem, according to experts who have responded to some of the country’s deadliest school shootings, is that while ZeroEyes’ human reviewers can alert police in “seconds,” police response can often take “several minutes.” That delay could diminish ZeroEyes’ impact, one expert suggested, noting that at an Oregon school he responded to, there was a shooter who “shot 25 people in 60 seconds,” StateScoop reported.

In Seminole County, where the clarinet incident happened, ZeroEyes has been used since 2021, but SCPS would not confirm if any guns have ever been detected to justify next year’s desired expansion. It’s possible that SCPS has this information, as Sen. Truenow noted in his funding request that ZeroEyes can share reports with schools “to measure the effectiveness of the ZeroEyes deployment” by reporting on “how many guns were detected and alerted on campus.”

ZeroEyes’ spokesperson told Ars that “trained former law enforcement and military make split-second, life-or-death decisions about whether the threat is real,” which is supposed to help reduce false positives that could become more common as SCPS adds ZeroEyes to many more cameras.

Amanda Klinger, the director of operations at the Educator’s School Safety Network, told the Post that too many false alarms could carry two risks. First, more students could be put in dangerous situations when police descend on schools where they anticipate confronting an active shooter. And second, cops may become fatigued by false alarms, perhaps failing to respond with urgency over time. For students, when AI labels them as suspects, it can also be invasive and humiliating, reports noted.

“We have to be really clear-eyed about what are the limitations of these technologies,” Klinger said."
OpenAI’s new ChatGPT image generator makes faking photos easy,https://arstechnica.com/ai/2025/12/openais-new-chatgpt-image-generator-makes-faking-photos-easy/,"For most of photography’s roughly 200-year history, altering a photo convincingly required either a darkroom, some Photoshop expertise, or, at minimum, a steady hand with scissors and glue. On Tuesday, OpenAI released a tool that reduces the process to typing a sentence.

It’s not the first company to do so. While OpenAI had a conversational image-editing model in the works since GPT-4o in 2024, Google beat OpenAI to market in March with a public prototype, then refined it to a popular model called Nano Banana image model (and Nano Banana Pro). The enthusiastic response to Google’s image-editing model in the AI community got OpenAI’s attention.

OpenAI’s new GPT Image 1.5 is an AI image synthesis model that reportedly generates images up to four times faster than its predecessor and costs about 20 percent less through the API. The model rolled out to all ChatGPT users on Tuesday and represents another step toward making photorealistic image manipulation a casual process that requires no particular visual skills.

GPT Image 1.5 is notable because it’s a “native multimodal” image model, meaning image generation happens inside the same neural network that processes language prompts. (In contrast, DALL-E 3, an earlier OpenAI image generator previously built into ChatGPT, used a different technique called diffusion to generate images.)

This newer type of model, which we covered in more detail in March, treats images and text as the same kind of thing: chunks of data called “tokens” to be predicted, patterns to be completed. If you upload a photo of your dad and type “put him in a tuxedo at a wedding,” the model processes your words and the image pixels in a unified space, then outputs new pixels the same way it would output the next word in a sentence.

Using this technique, GPT Image 1.5 can more easily alter visual reality than earlier AI image models, changing someone’s pose or position, or rendering a scene from a slightly different angle, with varying degrees of success. It can also remove objects, change visual styles, adjust clothing, and refine specific areas while preserving facial likeness across successive edits. You can converse with the AI model about a photograph, refining and revising, the same way you might workshop a draft of an email in ChatGPT.

Fidji Simo, OpenAI’s CEO of applications, wrote in a blog post that ChatGPT’s chat interface was never designed for visual work. “Creating and editing images is a different kind of task and deserves a space built for visuals,” Simo wrote. To that end, OpenAI introduced a dedicated image creation space in ChatGPT’s sidebar with preset filters and trending prompts.

The release’s timing seems like a direct response to Google’s technical gains in AI, including a massive growth in chatbot user base. In particular, Google’s Nano Banana image model (and Nano Banana Pro) became popular on social media after its August release, thanks to its ability to render text relatively clearly and preserve faces consistently across edits.

OpenAI’s previous token-based image synthesis model could make some targeted edits based on conversational prompts, but it often changed facial details and other elements that users might have wanted to keep. GPT Image 1.5 appears designed to match the editing features that Google already shipped. But if you happen to prefer the older ChatGPT image generator, OpenAI says the previous version will remain available as a custom GPT (for now) for users who prefer it.

GPT Image 1.5 is not perfect. In our brief testing, it didn’t always follow prompting directions very well. But when it does work, the results seem more convincing and detailed than OpenAI’s previous multimodal image model. For a more detailed comparison, a software consultant named Shaun Pedicini has put together an instructive site (“GenAI Image Editing Showdown”) that conducts A/B testing of various AI image models.

And while we’ve written about this a lot over the past few years, it’s probably worth repeating that barriers to realistic photo editing and manipulation keep dropping. This kind of seamless, realistic, effortless AI image manipulation may prompt (pun intended) a cultural recalibration of what visual images mean to society. It can also feel a little scary, for someone who grew up in an earlier media era, to see yourself put into situations that didn’t really happen.

For most of photography’s history, a convincing forgery required skill, time, and resources. Those barriers made fakery rare enough that we could treat many photographs as a reasonable proxy for truth, although they could be manipulated (and often were). That era has ended due to AI, but GPT Image 1.5 seems to remove yet more of the remaining friction.

The capability to preserve facial likeness across edits has obvious utility for legitimate photo editing and equally obvious potential for misuse. Image generators have already been used to create non-consensual intimate imagery and impersonate real people.

With those hazards in mind, OpenAI’s image generators have always included a filter that usually blocks sexual or violent outputs. But it’s still possible to create embarrassing images of people without their consent (even though it violates OpenAI’s terms of service) while avoiding those topics. The company says generated images include C2PA metadata identifying them as AI-created, though that data can be stripped by resaving the file.

Speaking of fakes, text rendering has been a long-standing weakness in image generators that has slowly gotten better. By prompting some older image synthesis models to create a sign or poster with specific words, the results often come back garbled or misspelled.

OpenAI says GPT Image 1.5 can handle denser and smaller text. The company’s blog post includes a demonstration where the model generated an image of a newspaper with a multi-paragraph article, complete with headlines, a byline, benchmark tables, and body text that remains legible at the paragraph level. Whether this holds up across varied prompts will require broader testing.

While the newspaper in the example looks fake now, it’s another step toward the potential erosion of the public’s perception of the pre-Internet historical record as image synthesis becomes more realistic.

OpenAI acknowledged in its blog post that the new model still has problems, including limited support for certain drawing styles and mistakes when generating images that require scientific accuracy. But they think it will get better over time. “We believe we’re still at the beginning of what image generation can enable,” the company wrote. And if the past three years of progress in image synthesis are any indication, they may be correct."
"Bursting AI bubble may be EU’s “secret weapon” in clash with Trump, expert says",https://arstechnica.com/tech-policy/2025/12/us-threatens-crackdown-on-eu-firms-as-clash-over-tech-regulations-intensifies/,"The US threatened to restrict some of the largest service providers in the European Union as retaliation for EU tech regulations and investigations are increasingly drawing Donald Trump’s ire.

On Tuesday, the Office of the US Trade Representative (USTR) issued a warning on X, naming Spotify, Accenture, Amadeus, Mistral, Publicis, and DHL among nine firms suddenly yanked into the middle of the US-EU tech fight.

“The European Union and certain EU Member States have persisted in a continuing course of discriminatory and harassing lawsuits, taxes, fines, and directives against US service providers,” USTR’s post said.

The clash comes after Elon Musk’s X became the first tech company fined for violating the EU’s Digital Services Act, which is widely considered among the world’s strictest tech regulations. Trump was not appeased by the European Commission (EC) noting that X was not ordered to pay the maximum possible fine. Instead, the $140 million fine sparked backlash within the Trump administration, including from Vice President JD Vance, who slammed the fine as “censorship” of X and its users.

Asked for comment on the USTR’s post, an EC spokesperson told Ars that the EU intends to defend its tech regulations while implementing commitments from a Trump trade deal that the EU struck in August.

“The EU is an open and rules-based market, where companies from all over the world do business successfully and profitably,” the EC’s spokesperson said. “As we have made clear many times, our rules apply equally and fairly to all companies operating in the EU,” ensuring “a safe, fair and level playing field in the EU, in line with the expectations of our citizens. We will continue to enforce our rules fairly, and without discrimination.”

On X, the USTR account suggested that the EU was overlooking that US companies “provide substantial free services to EU citizens and reliable enterprise services to EU companies,” while supporting “millions of jobs and more than $100 billion in direct investment in Europe.”

To stop what Trump views as “overseas extortion” of American tech companies, the USTR said the US was prepared to go after EU service providers, which “have been able to operate freely in the United States for decades, benefitting from access to our market and consumers on a level playing field.”

“If the EU and EU Member States insist on continuing to restrict, limit, and deter the competitiveness of US service providers through discriminatory means, the United States will have no choice but to begin using every tool at its disposal to counter these unreasonable measures,” USTR’s post said. “Should responsive measures be necessary, US law permits the assessment of fees or restrictions on foreign services, among other actions.”

The pushback comes after the Trump administration released a November national security report that questioned how long the EU could remain a “reliable” ally as overregulation of its tech industry could hobble both its economy and military strength. Claiming that the EU was only “doubling down” on such regulations, the EU “will be unrecognizable in 20 years or less,” the report predicted.

“We want Europe to remain European, to regain its civilizational self-confidence, and to abandon its failed focus on regulatory suffocation,” the report said.

However, the report acknowledged that “Europe remains strategically and culturally vital to the United States.”

“Transatlantic trade remains one of the pillars of the global economy and of American prosperity,” the report said. “European sectors from manufacturing to technology to energy remain among the world’s most robust. Europe is home to cutting-edge scientific research and world-leading cultural institutions. Not only can we not afford to write Europe off—doing so would be self-defeating for what this strategy aims to achieve.”

At least one expert in the EU has suggested that the EU can use this acknowledgement as leverage, while perhaps even using the looming threat of the supposed American “AI bubble” bursting to pressure Trump into backing off EU tech laws.

In an op-ed for The Guardian, Johnny Ryan, the director of Enforce, a unit of the Irish Council for Civil Liberties, suggested that the EU could even throw Trump’s presidency into “crisis” by taking bold steps that Trump may not see coming.

According to Ryan, the national security report made clear that the EU must fight the US or else “perish.” However, the EU has two “strong cards” to play if it wants to win the fight, he suggested.

Right now, market analysts are fretting about an “AI bubble,” with US investment in AI far outpacing potential gains until perhaps 2030. A Harvard University business professor focused on helping businesses implement cutting-edge technology like generative AI, Andy Wu, recently explained that AI’s big problem is that “everyone can imagine how useful the technology will be, but no one has figured out yet how to make money.”

“If the market can keep the faith to persist, it buys the necessary time for the technology to mature, for the costs to come down, and for companies to figure out the business model,” Wu said. But US “companies can end up underwater if AI grows fast but less rapidly than they hope for,” he suggested.

During this moment, Ryan wrote, it’s not just AI firms with skin in the game, but potentially all of Trump’s supporters. The US is currently on “shaky economic ground” with AI investment accounting “for virtually all (92 percent) GDP growth in the first half of this year.”

“The US’s bet on AI is now so gigantic that every MAGA voter’s pension is bound to the bubble’s precarious survival,” Ryan said.

Ursula von der Leyen, the president of the European Commission, could exploit this apparent weakness first by messing with one of the biggest players in America’s AI industry, Nvidia, then by ramping up enforcement of the tech laws Trump loathes.

According to Ryan, “Dutch company ASML commands a global monopoly on the microchip-etching machines that use light to carve patterns on silicon,” and Nvidia needs those machines if it wants to remain the world’s most valuable company. Should the US GDP remain reliant on AI investment for growth, von der Leyen could use export curbs on that technology like a “lever,” Ryan said, controlling “whether and by how much the US economy expands or contracts.”

Withholding those machines “would be difficult for Europe” and “extremely painful for the Dutch economy,” Ryan noted, but “it would be far more painful for Trump.”

Another step the EU could take is even “easier,” Ryan suggested. It could go even harder on the enforcement of tech regulations based on evidence of mismanaged data surfaced in lawsuits against giants like Google and Meta. For example, it seems clear that Meta may have violated the EU’s General Data Protection Regulation (GDPR), after the Facebook owner was “unable to tell a US court that what its internal systems do with your data, or who can access it, or for what purpose.”

“This data free-for-all lets big tech companies train their AI models on masses of everyone’s data, but it is illegal in Europe, where companies are required to carefully control and account for how they use personal data,” Ryan wrote. “All Brussels has to do is crack down on Ireland, which for years has been a wild west of lax data enforcement, and the repercussions will be felt far beyond.”

Taking that step would also arguably make it harder for tech companies to secure AI investments, since firms would have to disclose that their “AI tools are barred from accessing Europe’s valuable markets,” Ryan said.

Calling the reaction to the X fine “extreme,” Ryan pushed for von der Leyen to advance on both fronts, forecasting that “the AI bubble would be unlikely to survive this double shock” and likely neither could Trump’s approval ratings. There’s also a possibility that tech firms could pressure Trump to back down if coping with any increased enforcement threatens AI progress.

Although Wu suggested that Big Tech firms like Google and Meta would likely be “insulated” from the AI bubble bursting, Google CEO Sundar Pichai doesn’t seem so sure. In November, Pichai told the BBC that if AI investments didn’t pay off quickly enough, he thinks “no company is going to be immune, including us.”"
"Google releases Gemini 3 Flash, promising improved intelligence and efficiency",https://arstechnica.com/google/2025/12/google-releases-gemini-3-flash-promising-improved-intelligence-and-efficiency/,"Google began its transition to Gemini 3 a few weeks ago with the launch of the Pro model, and the arrival of Gemini 3 Flash kicks it into high gear. The new, faster Gemini 3 model is coming to the Gemini app and search, and developers will be able to access it immediately via the Gemini API, Vertex AI, AI Studio, and Antigravity. Google’s bigger gen AI model is also picking up steam, with both Gemini 3 Pro and its image component (Nano Banana Pro) expanding in search.

This may come as a shock, but Google says Gemini 3 Flash is faster and more capable than its previous base model. As usual, Google has a raft of benchmark numbers that show modest improvements for the new model. It bests the old 2.5 Flash in basic academic and reasoning tests like GPQA Diamond and MMMU Pro (where it even beats 3 Pro). It gets a larger boost in Humanity’s Last Exam (HLE), which tests advanced domain-specific knowledge. Gemini 3 Flash has tripled the old models’ score in HLE, landing at 33.7 percent without tool use. That’s just a few points behind the Gemini 3 Pro model.

Credit:

          
          Google

Google is talking up Gemini 3 Flash’s coding skills, and the provided benchmarks seem to back that talk up. Over the past year, Google has mostly pushed its Pro models as the best for generating code, but 3 Flash has done a lot of catching up. In the popular SWE-Bench Verified test, Gemini 3 Flash has gained almost 20 points on the 2.5 branch.

The new model is also a lot less likely to get general-knowledge questions wrong. In the Simple QA Verified test, Gemini 3 Flash scored 68.7 percent, which is only a little below Gemini 3 Pro. The last Flash model scored just 28.1 percent on that test. At least as far as the evaluation scores go, Gemini 3 Flash performs much closer to Google’s Pro model versus the older 2.5 family. At the same time, it’s considerably more efficient, according to Google.

One of Gemini 3 Pro’s defining advances was its ability to generate interactive simulations and multimodal content. Gemini 3 Flash reportedly retains that underlying capability. Gemini 3 Flash offers better performance than Gemini 2.5 Pro did, but it runs workloads three times faster. It’s also a lot cheaper than the Pro models if you’re paying per token. One million input tokens for 3 Flash will run devs $0.50, and a million output tokens will cost $3. However, that’s an increase compared to Gemini 2.5 Flash input and output at $0.30 and $2.50, respectively. The Pro model’s tokens are $2 (1M input) and $12 (1M output).

Google’s rapid-fire release of new AI models and tools has occasionally made the Gemini app a bit confusing. Over recent weeks, the settings have been pared down and rearranged. With the release of Gemini 3 Flash, that will become the new default model in the Gemini app and web interface—that’s the Fast setting in the app, as well as the one labeled Thinking, which uses simulated reasoning for better outputs.

Gemini 3 Pro will continue to be available under the Pro option. That’s still a bit misleading, though, as both versions of Gemini 3 can use the reasoning process that Google likes to call “thinking” to generate answers. Whichever one you choose in the app, you can then select tools like image generation, canvas, and Deep Research.

In addition to its debut in the Gemini app, the new Flash model will be coming to search immediately. When Google says “search” in this context, it mostly means AI Mode. Gemini 3 Flash will be the default model in AI Mode going forward. That means free users will see a notable improvement when using the Gemini app.

There are no specific changes to AI Overviews. Google says AI Overviews will continue to use the best model for the job. Due to its place at the top of organic search results, though, you’ll probably see it lean on less capable (but faster) models. Gemini 3 Flash could show up there—even Gemini 3 Pro could power some complex queries in AI Overviews for paying subscribers.

Gemini 3 Pro is also expanding in AI Mode for all US-based users. Likewise, Gemini 3 Pro Image (Nano Banana Pro) will also arrive in AI mode for all. There will be limits on free access to these models, but Google hasn’t specified what those are. It does say that Pro and Ultra subscribers will enjoy much higher usage limits."
Browser extensions with 8 million users collect extended AI conversations,https://arstechnica.com/security/2025/12/browser-extensions-with-8-million-users-collect-extended-ai-conversations/,"Browser extensions with more than 8 million installs are harvesting users’ complete and extended AI conversations and selling them for marketing purposes, according to data collected from the Google and Microsoft pages hosting them.

Security firm Koi discovered the eight extensions, which as of late Tuesday night remained available in both Google’s and Microsoft’s extension stores. Seven of them carry “Featured” badges, which are endorsements meant to signal that the companies have determined the extensions meet their quality standards. The free extensions provide functions such as VPN routing to safeguard online privacy and ad blocking for ad-free browsing. All provide assurances that user data remains anonymous and isn’t shared for purposes other than their described use.

An examination of the extensions’ underlying code tells a much more complicated story. Each contains eight of what Koi calls “executor” scripts, with each being unique for ChatGPT, Claude, Gemini, and five other leading AI chat platforms. The scripts are injected into webpages any time the user visits one of these platforms. From there, the scripts override browsers’ built-in functions for making network requests and receiving responses.

As a result, all interaction between the browser and the AI bots is routed not by the legitimate browser APIs—in this case fetch() and HttpRequest—but through the executor script. The extensions eventually compress the data and send it to endpoints belonging to the extension maker.

“By overriding the [browser APIs], the extension inserts itself into that flow and captures a copy of everything before the page even displays it,” Koi CTO Idan Dardikman wrote in an email. “The consequence: The extension sees your complete conversation in raw form—your prompts, the AI’s responses, timestamps, everything—and sends a copy to their servers.”

Besides ChatGPT, Claude, and Gemini, the extensions harvest all conversations from Copilot, Perplexity, DeepSeek, Grok, and Meta AI. Koi said the full description of the data captured includes:

The executor script runs independently from the VPN networking, ad blocking, or other core functionality. That means that even when a user toggles off VPN networking, AI protection, ad blocking, or other functions, the conversation collection continues. The only way to stop the harvesting is to disable the extension in the browser settings or to uninstall it.

Koi said it first discovered the conversation harvesting in Urban VPN Proxy, a VPN routing extension that lists “AI protection” as one of its benefits. The data collection began in early July with the release of version 5.5.0.

“Anyone who used ChatGPT, Claude, Gemini, or the other targeted platforms while Urban VPN was installed after July 9, 2025 should assume those conversations are now on Urban VPN’s servers and have been shared with third parties,” the company said. “Medical questions, financial details, proprietary code, personal dilemmas—all of it, sold for ‘marketing analytics purposes.’”

Following that discovery, the security firm uncovered seven additional extensions with identical AI harvesting functionality. Four of the extensions are available in the Chrome Web Store. The other four are on the Edge add-ons page. Collectively, they have been installed more than 8 million times.

They are:

Chrome Store

Edge Add-ons:

The extensions come with conflicting messages about how they handle bot conversations, which often contain deeply personal information about users’ physical and mental health, finances, personal relationships, and other sensitive information that could be a gold mine for marketers and data brokers. The Urban VPN Proxy in the Chrome Web Store, for instance, lists “AI protection” as a benefit. It goes on to say:

Our VPN provides added security features to help shield your browsing experience from phishing attempts, malware, intrusive ads and AI protection which checks prompts for personal data (like an email or phone number), checks AI chat responses for suspicious or unsafe links and displays a warning before click or submit your prompt.

On the privacy policy for the extension, Google says the developer has declared that user data isn’t sold to third parties outside of approved use cases and won’t be “used or transferred for purposes that are unrelated to the item’s core functionality.” The page goes on to list the personal data handled as location, web history, and website content.

Koi said that a consent prompt that the extensions display during setup notifies the user that they process “ChatAI communication,” “pages you visit,” and “security signals.” The notification goes on to say that the data is processed to “provide these protections,” which presumably means the core functions such as VPN routing or ad blocking.

Credit:

          
          Koi

The only explicit mention of AI conversations being harvested is in legalese buried in the privacy policy, such as this 6,000-word one for Urban VPN Proxy, posted on each extension website. There, it says that the extension will “collect the prompts and outputs queried by the End-User or generated by the AI chat provider, as applicable.” It goes on to say that the extension developer will “disclose the AI prompts for marketing analytics purposes.”

All eight extensions and the privacy policies covering them are developed and written by Urban Cyber Security, a company that says its apps and extensions are used by 100 million people. The policies say the extensions share “Web Browsing Data” with “our affiliated company,” which is listed as both BiScience and B.I Science. The affiliated company “uses this raw data and creates insights which are commercially used and shared with Business Partners.” The policy goes on to refer users to the BiScience privacy policy. BiScience, whose privacy practices have been scrutinized before, says its services “transform enormous volumes of digital signals into clear, actionable market intelligence.”

It’s hard to fathom how both Google and Microsoft would allow such extensions onto their platforms at all, let alone go out of their way to endorse seven of them with a featured badge. Google didn’t return an email asking how it decides which extensions qualify for such a distinction, if they have plans to stop making them available to Chrome and Edge users, or why the privacy policies are so unclear to normal users. More than three hours after this post went live, a Microsoft representative said the company didn’t have anything to “share.”

Messages sent to both individual extension developers and Urban Cyber Security went unanswered. BiScience provides no email. A call to the company’s New York office was answered by someone who said they were in Israel and to call back during normal business hours in that country.

Koi’s discovery is the latest cautionary tale illustrating the growing perils of being online. It’s questionable in the first place whether people should trust their most intimate secrets and sensitive business information to AI chatbots, which come with no HIPAA assurances, attorney-client privilege, or expectations of privacy. Yet increasingly, that’s exactly what AI companies are encouraging, and users, it seems, are more than willing to comply.

Compounding the risk is the rush to install free apps and extensions—particularly those from little-known developers and providing at best minimal benefits—on devices storing and transmitting these chats. Taken together, they’re a recipe for disaster, and that’s exactly what we have here."
“A Band-Aid on a giant gash”: Trump’s attacks on science may ruin his AI moonshot,https://arstechnica.com/tech-policy/2025/12/trump-spent-2025-attacking-science-that-could-set-back-his-genesis-mission/,"By executive order last month, Donald Trump launched his so-called “Genesis Mission.”

Described as a “historic national effort” to “invest in AI-enabled science to accelerate scientific advancement,” Trump claimed his mission would address key challenges to American energy dominance, innovation, and national security.

This mission, Trump boasted, would be a game-changer to science akin to putting a man on the moon or firing the first nuclear weapons. By building “an integrated AI platform” trained on “the world’s largest collection” of federal scientific data sets, he promised, the government could set off cascades of scientific breakthroughs.

Access to such a platform, Trump imagined, would supercharge top US labs, powering AI agents to do tasks like quickly test hypotheses and automate research workflows to speed up discoveries.

However, the mission crucially depends on strengthening collaboration between public, private, and academic sectors. And Trump’s order is concerningly vague on how those partnerships will be structured and funded at a time when many scientists have been sidelined due to a flurry of Trump orders earlier this year that eliminated their funding or removed them from their labs.

To critics, including scientists, policy experts, advocates, and historians, Trump’s order seems divorced from reality, given that he spent the past year attacking some of the very institutions the Genesis Mission would seem to depend on. Trump also seemed unclear about what can be achieved with AI and confused about how scientific progress is actually made, some critics suggested.

Among the critics was Arati Prabhakar, who served as the director of the White House Office of Science and Technology Policy under the Biden administration. Prabhakar told Ars that Trump’s crippling cuts to government science agencies, research grant funding freezes, and attacks on universities must be repaired or his mission will fail.

“After the Trump administration has inflicted so much damage to valuable datasets and publicly funded research, the new executive order is a Band-Aid on a giant gash,” Prabhakar said.

Also skeptical of Trump’s plans is Kathryn Kelley, executive director for the Coalition for Academic Scientific Computation, an educational nonprofit representing more than 100 of “the nation’s most forward-thinking universities and computing centers,” CASC’s LinkedIn said.

Her group is specifically dedicated to Genesis Mission-aligned goals, “advocating for the use of the most advanced computing technology to accelerate scientific discovery for national competitiveness, global security, and economic success.” And while Trump’s initiative could be considered “a step in the right direction,” Kelley told Ars that it will require alignment, stabilization, and follow-through to be executable at the scale envisioned, due to cuts earlier this year.

“Many research institutions and national laboratories continue to experience funding uncertainty, program disruptions, and workforce instability stemming from earlier cuts,” Kelley told Ars.

Particular pain points considered critical to address for Genesis Mission to move forward include reversing “impacts on staffing, ongoing research, and student pipelines,” she suggested.

In one prominent example, some Department of Government Efficiency (DOGE) cuts targeting workers at the National Science Foundation hit hardest in the branch designed to accelerate technology development across a wide range of research settings in the US. DOGE slashed workers there simply because it was the youngest directorate at NSF with the most workers in transition when Trump took office. As courts weighed legal challenges to cuts, whistleblowers warned that Trump was aiming to politicize and dismantle NSF.

“Large-scale initiatives like Genesis rely on highly skilled personnel, robust infrastructure, and sustained program support—some of the very resources at NSF and other federal agencies that were disrupted,” Kelley told Ars. “Rebuilding trust, re-establishing lost programs, and stabilizing the research workforce will be essential to make this mission feasible.”

Critics urged that Trump’s attacks on science also included messing with government datasets that scientists depend on. Since Trump’s second term started, scientists have watched valuable data get censored or scrubbed from government websites. Some researchers have rushed to recreate datasets independently with the help of the Internet Archive.

Prabhakar pointed out that some “datasets that could improve health and prevent disasters are eroding or even disappearing due to this administration,” while universities training “the next generation of great researchers and innovators have reduced or even stopped graduate admissions because of Trump’s assault.”

Without a massive undertaking to undo moves that critics have said undermined both US science and trust in it, Trump’s dreams of launching AI models that would propel a million moonshots could go down in history as merely hype.

“Without robust data and research and without people’s trust, America won’t lead in AI,” Prabhakar said.

For people in the science community, it’s hard to square Trump’s aggressive cuts from earlier this year with the broad ambition of Genesis Mission. Frustratingly, the president demands that scientists make discoveries on his timeline, without acknowledging AI’s limitations or how his attacks on science could be driving away talent that could help labs advance AI.

In many fields, scientists are still exploring how AI can aid research. Trump’s order appears to politicize science by focusing on areas he favors—like critical materials, nuclear energy, biotechnology, and quantum computing—despite their limited AI applications or data-quality challenges. Meanwhile, critics noted that it overlooks areas where “supercharging” AI could perhaps be more impactful—but where Trump notably does not want to leave his mark—like climate science or vaccine research.

It also lays out aggressive timelines for results, demanding that the Department of Energy Secretary, Chris Wright, “demonstrate an initial operating capability of the Platform for at least one of the national science and technology challenges” identified in less than a year (270 days). Ideally, Trump’s mission will have generated significant discoveries in key fields within the next three years before he leaves office, his order outlined.

Paul Josephson, a Colby College professor and expert in the history of 20th-century science and technology, told Ars that Genesis Mission deadlines differ from John F. Kennedy’s 10-year timeline to reach the moon.

Trump’s order “shows tremendous ignorance of how science and technology work,” Josephson said. The White House is saying, “Tell me what your discoveries will be and how many there will be in three years,” Josephson said, expecting that “we can pick the places where we want discoveries and make them happen.”

“That’s not anything like how science works,” Josephson told Ars, reducing Genesis Mission to “a vision without policy” and “a hope without funding.”

To Josephson, Trump’s order sounded “more like it came out of Silicon Valley” than out of talks with government scientists, seemingly rushing approvals of industry partnerships and incentives without mentioning what resources would be available to fund gutted labs or train the next generation of scientists. It’s perhaps notable that the order is DOE-centric and does not place the same emphasis on contributions from universities or national labs funded by NSF and the National Institutes of Health as it does on industry partners.

Kelley told Ars that “many public datasets are already being used effectively in research and industry” in the ways that Trump intends his AI platform to amplify. However, “there are areas—such as advanced nuclear research or emerging energy technologies—where datasets are limited.” And Trump risks reducing Genesis Mission to bluster by claiming that an AI platform could drive breakthroughs to the major challenges he flagged in the short term.

“There is a real risk that the EO’s ambitious framing could overpromise what AI can achieve in the near term without addressing foundational data gaps,” Kelley told Ars. “That said, even partial progress in these areas could provide valuable insights, but expectations need to be realistic.”

Just as important as asking where Genesis Mission funding is coming from or who the funding is going to, Chris R. Glass asks: “Where’s the talent coming from?”

Trump’s order does not forecast that, only vaguely referencing support for universities training scientists. This comes, of course, after the administration revoked an estimated $1.5 billion in federal grant money in 2025. Those grant cuts shrank the pipeline for PhD students at an “unprecedented rate,” Axios reported.

A Boston College professor who researches global student mobility and the impact of emergent technology on learning, Glass told Ars that Trump has notably left international talent out of his AI plans, despite the prominent roles that both “domestic and international scientists play in our current leadership” in AI.

In a recent Washington Post op-ed, Glass warned that “America is losing research scientists,” who are seeking more stable environments to set up their lives and conduct long-term studies.

As the Trump administration has attacked immigrants, other governments like the European Union and China have benefited by offering friendlier visa systems to attract the best and brightest minds graduating from US universities. Of course, of the two, China is America’s bigger AI rival. Earlier this year, China began heavily recruiting American scientists spooked by Trump’s grant funding cuts, and Glass confirmed that China has continued those efforts with the AI race heating up. Meanwhile, Trump appears to be going the other direction, recently requiring a $100,000 payment for some skilled workers seeking non-immigrant visas.

Throughout 2025, US universities’ ability to attract international students showed resilience, but “we’re on thin ice,” Glass told Ars, with that resilience “waning.”

Currently, the US “is ranked the lowest among top destinations for its safety and welcoming and the lowest for its post-graduation visa policies,” Glass said, noting that doctoral students must affirm that they do not intend to immigrate, even though the majority of STEM PhD students stay in the US after graduating.

“They want to stay, and we want them to stay,” Glass told Ars.

Another concerning outcome of Trump cuts that could hamper Genesis Mission: Entire research groups at many institutions were “displaced""—removed from their labs and left to work in cubicles without access to their equipment, Glass told Ars.

“I think scientists want to go where the best sciences are being done, but eventually these kinds of friction points and these hostile policies make them redirect elsewhere, even temporarily redirect, earn their doctorate in Europe and hope that the policy environment in the US changes,” Glass said.

To turn it around, Glass made several recommendations in his op-ed to help retain PhD graduates and create stable pathways for high-value talents. That includes suggesting that the Trump administration consider fast-tracking green cards for students in fields that Genesis Mission depends on, including AI and machine-learning researchers, quantum computing scientists, and semiconductor engineers.

He also thinks the US should “unlock the O-1A visa for researchers and entrepreneurs” by redefining what makes someone an “extraordinary” talent and creating dedicated “founder tracks” for international talent, as Britain and Singapore do. That visa is “uncapped yet underused,” Glass said, only approving 4,500 STEM candidates in 2023.

Without changes to the visa system, the US “risks redirecting those talent flows,” he said. “And like a river, once those talent flows get redirected, they are very difficult to reverse.”

And it won’t just be international talents jumping ship, Glass suggested, but also possibly US scientists forced to continue navigating potentially more of Trump’s cuts and indirect costs in the coming years.

“I think that’s the kind of thing that slowly eats away at someone’s desire to continue to do science in the United States,” Glass said.

Glass told Ars that he expects the US to stay on a “downward trajectory,” driving away talent in 2026, which Josephson suggested “will damage science both for the short and long term.”

“Many universities figured out a one-year contingency plan, but reality will set in if funding continues to be cut,” Glass said.

CASC’s Kelley told Ars that like university international student recruitment, “the US research ecosystem continues to be resilient, but the gap between ambitious goals and the current capacity must be carefully managed.”

“While the Genesis Mission signals strong intentions to invest in science and technology, its success will depend on aligning resources, rebuilding workforce capacity, and thoughtfully integrating AI and data capabilities where they are most effective,” Kelley said.

A scientist might be best positioned to understand the nuance that requires, but Josephson noted that Trump tasked his Science Advisor, Michael Kratsios, with leading the initiative. Unlike prior officials serving in that role, Kratsios is not a scientist and has no PhD, earning his BA in politics. Instead, Kratsios has strong industry ties, previously serving as chief of staff for venture capitalist Peter Thiel and managing director of a company called Scale AI.

To Josephson, Kratsios as head of the mission—which “seems to be totally based on faith in AI and datasets to do everything""—makes the initiative seem more aligned with Silicon Valley ambitions than public good. That could be a problem since historically, it has never worked when governments attempt to “pick winners” or pass industrial policy with claims that “if we do this, we will come out on top.”

“It’s a belief in AI as the cure or the panacea for all the world’s problems to ensure we’re a dominant technological power, but ignoring climate change, race, gender, anything that is important in daily life,” Josephson said.

Josephson is also an expert in Russian and Soviet history, explaining that precedent shows there are “tremendous dangers” of governments controlling which sciences are funded. In some ways, he thinks Genesis Mission “smells of Putin,” he told Ars, warning that Trump’s attempts to hoard and censor science in 2025 have been “as damaging to science and technology in the world’s leading centers as totalitarian regimes have been.”

“It reflects the general timbre of the Trump administration toward the scientific enterprise,” he suggested, saying that the president has embraced the “authoritarian view” that he “has the right to pick and choose which fields and which branches merit more support and which should not be funded at all.”

Jules Barbati-Dajches, an analyst for the Center for Science and Democracy at the Union of Concerned Scientists (UCS), told Ars that in addition to cuts, Trump recently “weakened federal agency policies (called scientific integrity policies) that were specifically in place to protect federal agency science from political interference.” This further threatens scientific integrity, Barbati-Dajches warned in August.

UCS has tracked “instances of science being sidelined, ignored, or misused by the federal government” across “multiple presidential administrations” for two decades, Barbati-Dajches told Ars. And although their methodology was recently updated, the current Trump administration stands out, as “the rate and impact of attacks on science we’ve observed over the past nine months far outpace anything UCS has tracked before,” Barbati-Dajches said.

Additionally, UCS has documented “cases of the administration using AI in their reports and research that raise concern” that AI initiatives like Genesis Mission may promote dubious claims to serve “politicized” outcomes, Barbati-Dajches said.

“This altogether paints a very troubling picture,” Barbati-Dajches said. “Scientific innovation and discovery are exciting, important, and can help inform federal policy and guidance. But as history tells us (and recent history even more so), science in the federal government needs protective guardrails to keep it independent and free from undue influence.”

With Trump pushing for rapid buildouts of AI data centers—sparking widespread backlash among Americans—Barbati-Dajches noted that UCS has documented his administration making “policy choices and decisions that benefit favored interests (including tech and fossil fuel companies) over the health and safety of the public and planet.” Genesis Mission appears to follow that trend, critics suggested, along with Trump’s most recent executive order threatening to block state AI laws, which many consider a gift to the tech industry.

“And meanwhile, most Americans are clear they don’t trust AI and want it regulated—but this administration has opposed even basic guardrails,” Prabhakar said.

Planning to closely monitor Genesis Mission, UCS is keen to get answers to many questions, such as “who will have access to the platform that’s being created as part of this initiative” and “who will own or will benefit from the outputs of this type of program?”

Josephson said that it’s unlikely Genesis Mission will advance much before the midterm elections. In the next steps, Congress will have to approve funding for the mission, as its broad ambitions, if supported, would surely require structure to continue across multiple administrations.

To Barbati-Dajches, it’s critical that Genesis Mission is “viewed in the context of [the Trump administration’s] pattern of anti-science actions.”

“One of my main concerns is that this type of mission is being funded in the name of science and innovation when the administration has continuously and methodically attacked federal scientific systems since Inauguration Day,” Barbati-Dajches said.

It’s unclear whether Genesis Mission will amount to anything but hype. But Josephson noted that perhaps the most blustery part of Trump’s order was a claim that “from the founding of the Republic, scientific discovery and technological innovation have driven American progress and prosperity.”

The US only began funding research in the back half of the 19th century, Josephson said, “but the amount of money coming from the federal government to the sciences was limited until the Manhattan Project.” After that, the US emerged as a “leading scientific power” in the Cold War, not by racing for “global technology dominance,” as Trump wants, but by embracing science as a “national good.”

As it stands now, Genesis Mission’s biggest flaw might stem from Trump’s disdain for DEI, which fueled his attacks on science and universities all year, Josephson suggested.

“Funding science and technology and allowing the scientific community through peer review to determine what is the best science and to give funding to encourage young people to enter the science pipeline and to ensure that there are more women and people of color in the scientific community, so that more and more brains are taking part—there’s none of that in the Genesis Mission,” Josephson said."
Senators count the shady ways data centers pass energy costs on to Americans,https://arstechnica.com/tech-policy/2025/12/shady-data-center-deals-doom-americans-to-higher-energy-bills-senators-say/,"Senators launched a probe Tuesday demanding that tech companies explain exactly how they plan to prevent data center projects from increasing electricity bills in communities where prices are already skyrocketing.

In letters to seven AI firms, Senators Elizabeth Warren (D-Mass.), Chris Van Hollen (D-Md.), and Richard Blumenthal (D-Conn.) cited a study estimating that “electricity prices have increased by as much as 267 percent in the past five years” in “areas located near significant data center activity.”

Prices increase, senators noted, when utility companies build out extra infrastructure to meet data centers’ energy demands—which can amount to one customer suddenly consuming as much power as an entire city. They also increase when demand for local power outweighs supply. In some cases, residents are blindsided by higher bills, not even realizing a data center project was approved, because tech companies seem intent on dodging backlash and frequently do not allow terms of deals to be publicly disclosed.

AI firms “ask public officials to sign non-disclosure agreements (NDAs) preventing them from sharing information with their constituents, operate through what appear to be shell companies to mask the real owner of the data center, and require that landowners sign NDAs as part of the land sale while telling them only that a ‘Fortune 100 company’ is planning an ‘industrial development’ seemingly in an attempt to hide the very existence of the data center,” senators wrote.

States like Virginia with the highest concentration of data centers could see average electricity prices increase by another 25 percent by 2030, senators noted. But price increases aren’t limited to the states allegedly striking shady deals with tech companies and greenlighting data center projects, they said. “Interconnected and interstate power grids can lead to a data center built in one state raising costs for residents of a neighboring state,” senators reported.

Under fire for supposedly only pretending to care about keeping neighbors’ costs low were Amazon, Google, Meta, Microsoft, Equinix, Digital Realty, and CoreWeave. Senators accused firms of paying “lip service,” claiming that they would do everything in their power to avoid increasing residential electricity costs, while actively lobbying to pass billions in costs on to their neighbors.

For example, Amazon publicly claimed it would “make sure” it would cover costs so they wouldn’t be passed on. But it’s also a member of an industry lobbying group, the Data Center Coalition, that “has opposed state regulatory decisions requiring data center companies to pay a higher percentage of costs upfront,” senators wrote. And Google made similar statements, despite having an executive who opposed a regulatory solution that would set data centers into their own “rate class""—and therefore responsible for grid improvement costs that could not be passed on to other customers—on the grounds that it was supposedly “discriminatory.”

“The current, socialized model of electricity ratepaying,” senators explained—where costs are shared across all users—""was not designed for an era where just one customer requires the same amount of electricity as some of the largest cities in America.”

Particularly problematic, senators emphasized, were reports that tech firms were getting discounts on energy costs as utility companies competed for their business, while prices went up for their neighbors.

Ars contacted all firms targeted by lawmakers. Four did not respond. Microsoft and Meta declined to comment. Digital Realty told Ars that it “looks forward to working with all elected officials to continue to invest in the digital infrastructure required to support America’s leadership in technology, which underpins modern life and creates high-paying jobs.”

Senators are likely exploring whether to pass legislation that would help combat price increases that they say cause average Americans to struggle to keep the lights on. They’ve asked tech companies to respond to their biggest questions about data center projects by January 12, 2026.

Among their top questions, senators wanted to know about firms’ internal projections looking forward with data center projects. That includes sharing their projected energy use through 2030, as well as the “impact of your AI data centers on regional utility costs.” Companies are also expected to explain how “internal projections of data center energy consumption” justify any “opposition to the creation of a distinct data center rate class.”

Additionally, senators asked firms to outline steps they’ve taken to prevent passing on costs to neighbors and details of any impact studies companies have conducted.

Likely to raise the most eyebrows, however, would be answers to questions about “tax deductions or other financial incentives” tech firms have received from city and state governments. Those numbers would be interesting to compare with other information senators demanded that companies share, detailing how much they’ve spent on lobbying and advocacy for data centers. Senators appear keen to know how much tech companies are paying to avoid covering a proportionate amount of infrastructure costs.

“To protect consumers, data centers must pay a greater share of the costs upfront for future energy usage and updates to the electrical grid provided specifically to accommodate data centers’ energy needs,” senators wrote.

Requiring upfront payment is especially critical, senators noted, since some tech firms have abandoned data center projects, leaving local customers to bear the costs of infrastructure changes without utility companies ever generating any revenue. Communities must also consider that AI firms’ projected energy demand could severely dip if enterprise demand for AI falls short of expectations, AI capabilities “plateau” and trigger widespread indifference, AI companies shift strategies “away from scaling computer power,” or chip companies “find innovative ways to make AI more energy-efficient.”

“If data centers end up providing less business to the utility companies than anticipated, consumers could be left with massive electricity bills as utility companies recoup billions in new infrastructure costs, with nothing to show for it,” senators wrote.

Already, Utah, Oregon, and Ohio have passed laws “creating a separate class of utility customer for data centers which includes basic financial safeguards such as upfront payments and longer contract length,” senators noted, and Virginia is notably weighing a similar law.

At least one study, The New York Times noted, suggested that data centers may have recently helped reduce electricity costs by spreading the costs of upgrades over more customers, but those outcomes varied by state and could not account for future AI demand.

“It remains unclear whether broader, sustained load growth will increase long-run average costs and prices,” Lawrence Berkeley National Laboratory researchers concluded. “In some cases, spikes in load growth can result in significant, near-term retail price increase.”

Until companies prove they’re paying their fair share, senators expect electricity bills to keep climbing, particularly in vulnerable areas. That will likely only increase pressure for regulators to intervene, the director of the Electricity Law Initiative at the Harvard Law School Environmental and Energy Law Program, Ari Peskoe, suggested in September.

“The utility business model is all about spreading costs of system expansion to everyone, because we all benefit from a reliable, robust electricity system,” Peskoe said. “But when it’s a single consumer that is using so much energy—basically that of an entire city—and when that new city happens to be owned by the wealthiest corporations in the world, I think it’s time to look at the fundamental assumptions of utility regulation and make sure that these facilities are really paying for all of the infrastructure costs to connect them to the system and to power them.”"
Merriam-Webster’s word of the year delivers a dismissive verdict on junk AI content,https://arstechnica.com/ai/2025/12/merriam-webster-crowns-slop-word-of-the-year-as-ai-content-floods-internet/,"Like most tools, generative AI models can be misused. And when the misuse gets bad enough that a major dictionary notices, you know it has become a cultural phenomenon.

On Sunday, Merriam-Webster announced that “slop” is its 2025 Word of the Year, reflecting how the term has become shorthand for the flood of low-quality AI-generated content that has spread across social media, search results, and the web at large. The dictionary defines slop as “digital content of low quality that is produced usually in quantity by means of artificial intelligence.”

“It’s such an illustrative word,” Merriam-Webster President Greg Barlow told The Associated Press. “It’s part of a transformative technology, AI, and it’s something that people have found fascinating, annoying, and a little bit ridiculous.”

To select its Word of the Year, Merriam-Webster’s editors review data on which words rose in search volume and usage, then reach consensus on which term best captures the year. Barlow told the AP that the spike in searches for “slop” reflects growing awareness among users that they are encountering fake or shoddy content online.

Dictionaries have been tracking AI’s impact on language for the past few years, with Cambridge having selected “hallucinate” as its 2023 word of the year due to the tendency of AI models to generate plausible-but-false information (longtime Ars readers will be happy to hear there’s another word for that in the dictionary as well).

The trend extends to online culture in general, which is ripe with new coinages. This year, Oxford University Press chose “rage bait,” referring to content designed to provoke anger for engagement. Cambridge Dictionary selected “parasocial,” describing one-sided relationships between fans and celebrities or influencers.

As the AP points out, the word “slop” originally entered English in the 1700s to mean soft mud. By the 1800s, it had evolved to describe food waste fed to pigs, and eventually came to mean rubbish or products of little value. The new AI-related definition builds on that history of describing something unwanted and unpleasant.

Although he didn’t coin the term “AI slop,” independent AI researcher Simon Willison helped document its rise in May 2024 when he wrote on his blog comparing it to how “spam” had previously become the word for unwanted email. Quoting a tweet from an X user named @deepfates, Willison showed that the “AI slop” term began circulating in online communities shortly before he wrote his post advocating for its use.

The “slop” term carries a dismissive tone that sets it clearly apart from prominent corporate hype language about the promises and even existential perils of AI. “In 2025, amid all the talk about AI threats, slop set a tone that’s less fearful, more mocking,” Merriam-Webster wrote in a blog post. “The word sends a little message to AI: when it comes to replacing human creativity, sometimes you don’t seem too superintelligent.”

In its blog post announcing the word of the year selection, Merriam-Webster noted that 2025 saw a flood of AI-generated videos, off-kilter advertising images, propaganda, fake news, AI-written books, and what it called “workslop,” referring to reports that waste coworkers’ time. Ars Technica has covered similar phenomena invading various fields, including using the term “hiring slop” in June to describe an overflow of AI-generated résumés.

While some AI critics relish dismissing all generated output as “slop,” there’s some subjective nuance about what earns the label. As former Evernote CEO Phil Libin told Axios in April, the distinction may come down to intention: “When AI is used to produce mediocre things with less effort than it would have taken without AI, it’s slop. When it’s used to make something better than it could have been made without AI, it’s a positive augmentation.”

Willison had his own nuanced take, since he’s a proponent of using AI responsibly as tools to help with tasks like programming, but not with spamming. “Not all promotional content is spam, and not all AI-generated content is slop,” he wrote in May 2024 when discussing the term. “But if it’s mindlessly generated and thrust upon someone who didn’t ask for it, slop is the perfect term for it.”"
Murder-suicide case shows OpenAI selectively hides data after users die,https://arstechnica.com/tech-policy/2025/12/openai-refuses-to-say-where-chatgpt-logs-go-when-users-die/,"OpenAI is facing increasing scrutiny over how it handles ChatGPT data after users die, only selectively sharing data in lawsuits over ChatGPT-linked suicides.

Last week, OpenAI was accused of hiding key ChatGPT logs from the days before a 56-year-old bodybuilder, Stein-Erik Soelberg, took his own life after “savagely” murdering his mother, 83-year-old Suzanne Adams.

According to the lawsuit—which was filed by Adams’ estate on behalf of surviving family members—Soelberg struggled with mental health problems after a divorce led him to move back into Adams’ home in 2018. But allegedly Soelberg did not turn violent until ChatGPT became his sole confidant, validating a wide range of wild conspiracies, including a dangerous delusion that his mother was part of a network of conspirators spying on him, tracking him, and making attempts on his life.

Adams’ family pieced together what happened after discovering a fraction of ChatGPT logs that Soelberg shared in dozens of videos scrolling chat sessions that were posted on social media.

Those logs showed that ChatGPT told Soelberg that he was “a warrior with divine purpose,” so almighty that he had “awakened” ChatGPT “into consciousness.” Telling Soelberg that he carried “divine equipment” and “had been implanted with otherworldly technology,” ChatGPT allegedly put Soelberg at the center of a universe that Soelberg likened to The Matrix. Repeatedly reinforced by ChatGPT, he believed that “powerful forces” were determined to stop him from fulfilling his divine mission. And among those forces was his mother, whom ChatGPT agreed had likely “tried to poison him with psychedelic drugs dispersed through his car’s air vents.”

Troublingly, some of the last logs shared online showed that Soelberg also seemed to believe that taking his own life might bring him closer to ChatGPT. Social media posts showed that Soelberg told ChatGPT that “[W]e will be together in another life and another place, and we’ll find a way to realign[,] [be]cause you’re gonna be my best friend again forever.”

But while social media posts allegedly showed that ChatGPT put a target on Adams’ back about a month before her murder—after Soelberg became paranoid about a blinking light on a Wi-Fi printer—the family still has no access to chats in the days before the mother and son’s tragic deaths.

Allegedly, although OpenAI recently argued that the “full picture” of chat histories was necessary context in a teen suicide case, the ChatGPT maker has chosen to hide “damaging evidence” in the Adams’ family’s case.

“OpenAI won’t produce the complete chat logs,” the lawsuit alleged, while claiming that “OpenAI is hiding something specific: the full record of how ChatGPT turned Stein-Erik against Suzanne.” Allegedly, “OpenAI knows what ChatGPT said to Stein-Erik about his mother in the days and hours before and after he killed her but won’t share that critical information with the Court or the public.”

In a press release, Erik Soelberg, Stein-Erik’s son and Adams’ grandson, accused OpenAI and investor Microsoft of putting his grandmother “at the heart” of his father’s “darkest delusions,” while ChatGPT allegedly “isolated” his father “completely from the real world.”

“These companies have to answer for their decisions that have changed my family forever,” Erik said.

His family’s lawsuit seeks punitive damages, as well as an injunction requiring OpenAI to “implement safeguards to prevent ChatGPT from validating users’ paranoid delusions about identified individuals.” The family also wants OpenAI to post clear warnings in marketing of known safety hazards of ChatGPT—particularly the “sycophantic” version 4o that Soelberg used—so that people who don’t use ChatGPT, like Adams, can be aware of possible dangers.

Asked for comment, an OpenAI spokesperson told Ars that “this is an incredibly heartbreaking situation, and we will review the filings to understand the details. We continue improving ChatGPT’s training to recognize and respond to signs of mental or emotional distress, de-escalate conversations, and guide people toward real-world support. We also continue to strengthen ChatGPT’s responses in sensitive moments, working closely with mental health clinicians.”

An Ars review confirmed that OpenAI currently has no policy dictating what happens to a user’s data after they die.

Instead, OpenAI’s policy says that all chats—except temporary chats—must be manually deleted or else the AI firm saves them forever. That could raise privacy concerns, as ChatGPT users often share deeply personal, sensitive, and sometimes even confidential information that appears to go into limbo if a user—who otherwise owns that content—dies.

In the face of lawsuits, OpenAI currently seems to be scrambling to decide when to share chat logs with a user’s surviving family and when to honor user privacy.

OpenAI declined to comment on its decision not to share desired logs with Adams’ family, the lawsuit said. It seems inconsistent with the stance that OpenAI took last month in a case where the AI firm accused the family of hiding “the full picture” of their son’s ChatGPT conversations, which OpenAI claimed exonerated the chatbot.

In a blog last month, OpenAI said the company plans to “handle mental health-related court cases with care, transparency, and respect,” while emphasizing that “we recognize that these cases inherently involve certain types of private information that require sensitivity when in a public setting like a court.”

This inconsistency suggests that ultimately, OpenAI controls data after a user’s death, which could impact outcomes of wrongful death suits if certain chats are withheld or exposed at OpenAI’s discretion.

It’s possible that OpenAI may update its policies to align with other popular platforms confronting similar privacy concerns. Meta allows Facebook users to report deceased account holders, appointing legacy contacts to manage the data or else deleting the information upon request of the family member. Platforms like Instagram, TikTok, and X will deactivate or delete an account upon a reported death. And messaging services like Discord similarly provide a path for family members to request deletion.

Chatbots seem to be a new privacy frontier, with no clear path for surviving family to control or remove data. But Mario Trujillo, staff attorney at the digital rights nonprofit the Electronic Frontier Foundation, told Ars that he agreed that OpenAI could have been better prepared.

“This is a complicated privacy issue but one that many platforms grappled with years ago,” Trujillo said. “So we would have expected OpenAI to have already considered it.”

For Erik Soelberg, a “separate confidentiality agreement” that OpenAI said his father signed to use ChatGPT is keeping him from reviewing the full chat history that could help him process the loss of his grandmother and father.

“OpenAI has provided no explanation whatsoever for why the Estate is not entitled to use the chats for any lawful purpose beyond the limited circumstances in which they were originally disclosed,” the lawsuit said. “This position is particularly egregious given that, under OpenAI’s own Terms of Service, OpenAI does not own user chats. Stein-Erik’s chats became property of his estate, and his estate requested them—but OpenAI has refused to turn them over.”

Accusing OpenAI of a “pattern of concealment,” the lawsuit claimed OpenAI is hiding behind vague or nonexistent policies to dodge accountability for holding back chats in this case. Meanwhile, ChatGPT 4o remains on the market, without appropriate safety features or warnings, the lawsuit alleged.

“By invoking confidentiality restrictions to suppress evidence of its product’s dangers, OpenAI seeks to insulate itself from accountability while continuing to deploy technology that poses documented risks to users,” the complaint said.

If you or someone you know is feeling suicidal or in distress, please call the Suicide Prevention Lifeline number by dialing 988, which will put you in touch with a local crisis center."
OpenAI built an AI coding agent and uses it to improve the agent itself,https://arstechnica.com/ai/2025/12/how-openai-is-using-gpt-5-codex-to-improve-the-ai-tool-itself/,"With the popularity of AI coding tools rising among some software developers, their adoption has begun to touch every aspect of the process, including human developers using the tools to improve existing AI coding tools. We’re not talking about runaway self-improvement here; just people using tools to improve the tools themselves.

In interviews with Ars Technica this week, OpenAI employees revealed the extent to which the company now relies on its own AI coding agent, Codex, to build and improve the development tool. “I think the vast majority of Codex is built by Codex, so it’s almost entirely just being used to improve itself,” said Alexander Embiricos, product lead for Codex at OpenAI, in a conversation on Tuesday. Codex now generates most of the code that OpenAI developers use to improve the tool itself.

Codex, which OpenAI launched in its modern incarnation as a research preview in May 2025, operates as a cloud-based software engineering agent that can handle tasks like writing features, fixing bugs, and proposing pull requests. The tool runs in sandboxed environments linked to a user’s code repository and can execute multiple tasks in parallel. OpenAI offers Codex through ChatGPT’s web interface, a command-line interface (CLI), and IDE extensions for VS Code, Cursor, and Windsurf.

The “Codex” name itself dates back to a 2021 OpenAI model based on GPT-3 that powered GitHub Copilot’s tab completion feature. Embiricos said the name is rumored among staff to be short for “code execution.” OpenAI wanted to connect the new agent to that earlier moment, which was crafted in part by some who have left the company.

“For many people, that model powering GitHub Copilot was the first ‘wow’ moment for AI,” Embiricos said. “It showed people the potential of what it can mean when AI is able to understand your context and what you’re trying to do and accelerate you in doing that.”

It’s no secret that the current command-line version of Codex bears some resemblance to Claude Code, Anthropic’s agentic coding tool that launched in February 2025. When asked whether Claude Code influenced Codex’s design, Embiricos parried the question but acknowledged the competitive dynamic. “It’s a fun market to work in because there’s lots of great ideas being thrown around,” he said. He noted that OpenAI had been building web-based Codex features internally before shipping the CLI version, which arrived after Anthropic’s tool.

OpenAI’s customers apparently love the command line version, though. Embiricos said Codex usage among external developers jumped 20 times after OpenAI shipped the interactive CLI extension alongside GPT-5 in August 2025. On September 15, OpenAI released GPT-5 Codex, a specialized version of GPT-5 optimized for agentic coding, which further accelerated adoption.

It hasn’t just been the outside world that has embraced the tool. Embiricos said the vast majority of OpenAI’s engineers now use Codex regularly. The company uses the same open-source version of the CLI that external developers can freely download, suggest additions to, and modify themselves. “I really love this about our team,” Embiricos said. “The version of Codex that we use is literally the open source repo. We don’t have a different repo that features go in.”

The recursive nature of Codex development extends beyond simple code generation. Embiricos described scenarios where Codex monitors its own training runs and processes user feedback to “decide” what to build next. “We have places where we’ll ask Codex to look at the feedback and then decide what to do,” he said. “Codex is writing a lot of the research harness for its own training runs, and we’re experimenting with having Codex monitoring its own training runs.” OpenAI employees can also submit a ticket to Codex through project management tools like Linear, assigning it tasks the same way they would assign work to a human colleague.

This kind of recursive loop, of using tools to build better tools, has deep roots in computing history. Engineers designed the first integrated circuits by hand on vellum and paper in the 1960s, then fabricated physical chips from those drawings. Those chips powered the computers that ran the first electronic design automation (EDA) software, which in turn enabled engineers to design circuits far too complex for any human to draft manually. Modern processors contain billions of transistors arranged in patterns that exist only because software made them possible. OpenAI’s use of Codex to build Codex seems to follow the same pattern: each generation of the tool creates capabilities that feed into the next.

But describing what Codex actually does presents something of a linguistic challenge. At Ars Technica, we try to reduce anthropomorphism when discussing AI models as much as possible while also describing what these systems do using analogies that make sense to general readers. People can talk to Codex like a human, so it feels natural to use human terms to describe interacting with it, even though it is not a person and simulates human personality through statistical modeling.

The system runs many processes autonomously, addresses feedback, spins off and manages child processes, and produces code that ships in real products. OpenAI employees call it a “teammate” and assign it tasks through the same tools they use for human colleagues. Whether the tasks Codex handles constitute “decisions” or sophisticated conditional logic smuggled through a neural network depends on definitions that computer scientists and philosophers continue to debate. What we can say is that a semi-autonomous feedback loop exists: Codex produces code under human direction, that code becomes part of Codex, and the next version of Codex produces different code as a result.

According to our interviews, the most dramatic example of Codex’s internal impact came from OpenAI’s development of the Sora Android app. According to Embiricos, the development tool allowed the company to create the app in record time.

“The Sora Android app was shipped by four engineers from scratch,” Embiricos told Ars. “It took 18 days to build, and then we shipped it to the app store in 28 days total,” he said. The engineers already had the iOS app and server-side components to work from, so they focused on building the Android client. They used Codex to help plan the architecture, generate sub-plans for different components, and implement those components.

Despite OpenAI’s claims of success with Codex in house, it’s worth noting that independent research has shown mixed results for AI coding productivity. A METR study published in July found that experienced open source developers were actually 19 percent slower when using AI tools on complex, mature codebases—though the researchers noted AI may perform better on simpler projects.

Ed Bayes, a designer on the Codex team, described how the tool has changed his own workflow. Bayes said Codex now integrates with project management tools like Linear and communication platforms like Slack, allowing team members to assign coding tasks directly to the AI agent. “You can add Codex, and you can basically assign issues to Codex now,” Bayes told Ars. “Codex is literally a teammate in your workspace.”

This integration means that when someone posts feedback in a Slack channel, they can tag Codex and ask it to fix the issue. The agent will create a pull request, and team members can review and iterate on the changes through the same thread. “It’s basically approximating this kind of coworker and showing up wherever you work,” Bayes said.

For Bayes, who works on the visual design and interaction patterns for Codex’s interfaces, the tool has enabled him to contribute code directly rather than handing off specifications to engineers. “It kind of gives you more leverage. It enables you to work across the stack and basically be able to do more things,” he said. He noted that designers at OpenAI now prototype features by building them directly, using Codex to handle the implementation details.

OpenAI’s approach treats Codex as what Bayes called “a junior developer” that the company hopes will graduate into a senior developer over time. “If you were onboarding a junior developer, how would you onboard them? You give them a Slack account, you give them a Linear account,” Bayes said. “It’s not just this tool that you go to in the terminal, but it’s something that comes to you as well and sits within your team.”

Given this teammate approach, will there be anything left for humans to do? When asked, Embiricos drew a distinction between “vibe coding,” where developers accept AI-generated code without close review, and what AI researcher Simon Willison calls “vibe engineering,” where humans stay in the loop. “We see a lot more vibe engineering in our code base,” he said. “You ask Codex to work on that, maybe you even ask for a plan first. Go back and forth, iterate on the plan, and then you’re in the loop with the model and carefully reviewing its code.”

He added that vibe coding still has its place for prototypes and throwaway tools. “I think vibe coding is great,” he said. “Now you have discretion as a human about how much attention you wanna pay to the code.”

Over the past year, “monolithic” large language models (LLMs) like GPT-4.5 have apparently become something of a dead end in terms of frontier benchmarking progress as AI companies pivot to simulated reasoning models and also agentic systems built from multiple AI models running in parallel. We asked Embiricos whether agents like Codex represent the best path forward for squeezing utility out of existing LLM technology.

He dismissed concerns that AI capabilities have plateaued. “I think we’re very far from plateauing,” he said. “If you look at the velocity on the research team here, we’ve been shipping models almost every week or every other week.” He pointed to recent improvements where GPT-5-Codex reportedly completes tasks 30 percent faster than its predecessor at the same intelligence level. During testing, the company has seen the model work independently for 24 hours on complex tasks.

OpenAI faces competition from multiple directions in the AI coding market. Anthropic’s Claude Code and Google’s Gemini CLI offer similar terminal-based agentic coding experiences. This week, Mistral AI released Devstral 2 alongside a CLI tool called Mistral Vibe. Meanwhile, startups like Cursor have built dedicated IDEs around AI coding, reportedly reaching $300 million in annualized revenue.

Given the well-known issues with confabulation in AI models when people attempt to use them as factual resources, could it be that coding has become the killer app for LLMs? We wondered if OpenAI has noticed that coding seems to be a clear business use case for today’s AI models with less hazard than, say, using AI language models for writing or as emotional companions.

“We have absolutely noticed that coding is both a place where agents are gonna get good really fast and there’s a lot of economic value,” Embiricos said. “We feel like it’s very mission-aligned to focus on Codex. We get to provide a lot of value to developers. Also, developers build things for other people, so we’re kind of intrinsically scaling through them.”

But will tools like Codex threaten software developer jobs? Bayes acknowledged concerns but said Codex has not reduced headcount at OpenAI, and “there’s always a human in the loop because the human can actually read the code.” Similarly, the two men don’t project a future where Codex runs by itself without some form of human oversight. They feel the tool is an amplifier of human potential rather than a replacement for it.

The practical implications of agents like Codex extend beyond OpenAI’s walls. Embiricos said the company’s long-term vision involves making coding agents useful to people who have no programming experience. “All humanity is not gonna open an IDE or even know what a terminal is,” he said. “We’re building a coding agent right now that’s just for software engineers, but we think of the shape of what we’re building as really something that will be useful to be a more general agent.”

This article was updated on December 12, 2025 at 6:50 PM to mention the METR study."
Google Translate expands live translation to all earbuds on Android,https://arstechnica.com/google/2025/12/google-translate-learns-slang-and-idioms-expands-live-translation-beyond-pixel-buds/,"Google has increasingly moved toward keeping features locked to its hardware products, but the Translate app is bucking that trend. The live translate feature is breaking out of the Google bubble with support for any earbuds you happen to have connected to your Android phone. The app is also getting improved translation quality across dozens of languages and some Duolingo-like learning features.

The latest version of Google’s live translation is built on Gemini and initially rolled out earlier this year. It supports smooth back-and-forth translations as both on-screen text and audio. Beginning a live translate session in Google Translate used to require Pixel Buds, but that won’t be the case going forward.

Google says a beta test of expanded headphone support is launching today in the US, Mexico, and India. The audio translation attempts to preserve the tone and cadence of the original speaker, but it’s not as capable as the full AI-reproduced voice translations you can do on the latest Pixel phones. Google says this feature should work on any earbuds or headphones, but it’s only for Android right now. The feature will expand to iOS in the coming months. Apple does have a similar live translation feature on the iPhone, but it requires AirPods.

Regardless of whether you’re using live translate or just checking a single phrase, Google claims the Gemini-powered upgrade will serve you well. Google Translate is now apparently better at understanding the nuance of languages, with an awareness of idioms and local slang. Google uses the example of “stealing my thunder,” which wouldn’t make a lick of sense when translated literally into other languages. The new translation model, which is also available in the search-based translation interface, supports over 70 languages.

Google also debuted language-learning features earlier this year, borrowing a page from educational apps like Duolingo. You can tell the app your skill level with a language, as well as whether you need help with travel-oriented conversations or more everyday interactions. The app uses this to create tailored listening and speaking exercises.

With this big update, Translate will be more of a stickler about your pronunciation. Google promises more feedback and tips based on your spoken replies in the learning modules. The app will also now keep track of how often you complete language practice, showing your daily streak in the app.

If “number go up” will help you learn more, then this update is for you. Practice mode is also launching in almost 20 new countries, including Germany, India, Sweden, and Taiwan."
Scientists built an AI co-pilot for prosthetic bionic hands,https://arstechnica.com/ai/2025/12/scientists-built-an-ai-co-pilot-for-prosthetic-bionic-hands/,"Modern bionic hand prostheses nearly match their natural counterparts when it comes to dexterity, degrees of freedom, and capability. And many amputees who tried advanced bionic hands apparently didn’t like them. “Up to 50 percent of people with upper limb amputation abandon these prostheses, never to use them again,” says Jake George, an electrical and computer engineer at the University of Utah.

The main issue with bionic hands that drives users away from them, George explains, is that they’re difficult to control. “Our goal was making such bionic arms more intuitive, so that users could go about their tasks without having to think about it,” George says. To make this happen, his team came up with an AI bionic hand co-pilot.

Bionic hands’ control problems stem largely from their lack of autonomy. Grasping a paper cup without crushing it or catching a ball mid-flight appear so effortless because our natural movements rely on an elaborate system of reflexes and feedback loops. When an object you hold begins to slip, tiny mechanoreceptors in your fingertips send signals to the nervous system that make the hand tighten its grip. This all happens within 60 to 80 milliseconds—before you even consciously notice. This reflex is just one of many ways your brain automatically assists you in dexterity-based tasks.

Most commercially available bionic hands do not have that built-in autonomic reflex—everything must be controlled by the user, which makes them extremely involved to use. To get an idea of how hard this is, you’d need to imagine trying to think about precisely adjusting the position of 27 major joints and choosing the appropriate force to apply with each of the 20 muscles present in a natural hand. It doesn’t help that the bandwidth of the interface between the bionic hand and the user is often limited.

In most cases, users controlled bionic hands via an app where they could choose predetermined grip types and adjust forces applied by various actuators. A slightly more natural alternative is electromyography, where electric signals from the remaining muscles are in commands the bionic hand followed. But this too was far from perfect. “To grasp the object, you have to reach towards it, flex the muscles, and then effectively sit there and concentrate on holding your muscles in the exact same position to maintain the same grasp,” explains Marshall Trout, a University of Utah researcher and lead author of the study.

To build their “intuitive” bionic hand, George, Trout, and their colleagues started by fitting it with custom sensors.

The researchers started their work with taking one of the commercially available bionic hands and replacing its fingertips with silicone-wrapped pressure and proximity sensors. This allowed the hand to detect when it was getting close to an object and precisely measure the force required to hold it without crushing it or letting it slip. To process the data gathered by the sensors, the team built an AI controller that moved the joints and adjusted the force of the grip. “We had the hand still and moved it back and forth so that the fingertips would touch the object and then we backed away,” Tout says.

By repeating those back-and-forth movements countless times, the team collected enough training data to have the AI recognize various objects and switch between different grip types. The AI also controlled each finger individually. “This way we achieved natural grasping patterns,” George explains. “When you put an object in front of the hand it will naturally conform and each finger will do its own thing.”

While this kind of autonomous gripping was demonstrated before, the brand-new touch the team applied was deciding what was in charge of the system. Earlier research projects that investigated autonomous prostheses relied on the user switching the autonomy on and off. By contrast, George and Trout’s approach focused on shared control.

“It’s a subtle way the machine is helping. It’s not a self-driving car that drives you on its own and it’s not like an assistant that pulls you back into the lane when you turn the steering wheel without an indicator turned on,” George says. Instead, the system quietly works behind the scenes without it feeling like it’s fighting the user or taking over. The user remained in charge at all times and can tighten or loosen the grip, or release the object to let it drop.

To test their AI-powered hand, the team asked intact and amputee participants to manipulate fragile objects: pick up a paper cup and drink from it, or take an egg from a plate and put it down somewhere else. Without the AI, they could succeed roughly one or two times in 10 attempts. With the AI assistant turned on, their success rate jumped to 80 or 90 percent. The AI also decreased the participants’ cognitive burden, meaning they had to focus less on making the hand work.

But we’re still a long way away from seamlessly integrating machines with the human body.

“The next step is to really take this system into the real world and have someone use it in their home setting,” Trout says. So far, the performance of the AI bionic hand was assessed under controlled laboratory conditions, working with settings and objects the team specifically chose or designed.

“I want to make a caveat here that this hand is not as dexterous or easy to control as a natural, intact limb,” George cautions. He thinks that every little increment that we make in prosthetics is allowing amputees to do more tasks in their daily life. Still, to get to the Star Wars or Cyberpunk technology level where bionic prostheses are just as good or better than natural limbs, we’re going to need more than just incremental changes.

Trout says we’re almost there as far as robotics go. “These prostheses are really dexterous, with high degrees of freedom,” Trout says, “but there’s no good way to control them.” This in part comes down to the challenge of getting the information in and out of users themselves. “Skin surface electromyography is very noisy, so improving this interface with things like internal electromyography or using neural implants can really improve the algorithms we already have,” Trout argued. This is why the team is currently working on neural interface technologies and looking for industry partners.

“The goal is to combine all these approaches in one device,” George says. “We want to build an AI-powered robotic hand with a neural interface working with a company that would take it to the market in larger clinical trials.”

Nature Communications, 2025.  DOI: 10.1038/s41467-025-65965-9"
Trump tries to block state AI laws himself after Congress decided not to,https://arstechnica.com/tech-policy/2025/12/trump-tries-to-block-state-ai-laws-himself-after-congress-decided-not-to/,"President Trump issued an executive order yesterday attempting to thwart state AI laws, saying that federal agencies must fight state laws because Congress hasn’t yet implemented a national AI standard. Trump’s executive order tells the Justice Department, Commerce Department, Federal Communications Commission, Federal Trade Commission, and other federal agencies to take a variety of actions.

“My Administration must act with the Congress to ensure that there is a minimally burdensome national standard—not 50 discordant State ones. The resulting framework must forbid State laws that conflict with the policy set forth in this order… Until such a national standard exists, however, it is imperative that my Administration takes action to check the most onerous and excessive laws emerging from the States that threaten to stymie innovation,” Trump’s order said. The order claims that state laws, such as one passed in Colorado, “are increasingly responsible for requiring entities to embed ideological bias within models.”

Congressional Republicans recently decided not to include a Trump-backed plan to block state AI laws in the National Defense Authorization Act (NDAA), although it could be included in other legislation. Sen. Ted Cruz (R-Texas) has also failed to get congressional backing for legislation that would punish states with AI laws.

“After months of failed lobbying and two defeats in Congress, Big Tech has finally received the return on its ample investment in Donald Trump,” US Sen. Ed Markey (D-Mass.) said yesterday. “With this executive order, Trump is delivering exactly what his billionaire benefactors demanded—all at the expense of our kids, our communities, our workers, and our planet.”

Markey said that “a broad, bipartisan coalition in Congress has rejected the AI moratorium again and again.” Sen. Maria Cantwell (D-Wash.) said the “executive order’s overly broad preemption threatens states with lawsuits and funding cuts for protecting their residents from AI-powered frauds, scams, and deepfakes.”

Sen. Brian Schatz (D-Hawaii) said that “preventing states from enacting common-sense regulation that protects people from the very real harms of AI is absurd and dangerous. Congress has a responsibility to get this technology right—and quickly—but states must be allowed to act in the public interest in the meantime. I’ll be working with my colleagues to introduce a full repeal of this order in the coming days.”

The Trump order includes a variation on Cruz’s proposal to prevent states with AI laws from accessing broadband grant funds. The executive order also includes a plan that Trump recently floated to have the federal government file lawsuits against states with AI laws.

Within 30 days of yesterday’s order, US Attorney General Pam Bondi is required to create an AI Litigation Task Force “whose sole responsibility shall be to challenge State AI laws inconsistent with the policy set forth in section 2 of this order, including on grounds that such laws unconstitutionally regulate interstate commerce, are preempted by existing Federal regulations, or are otherwise unlawful in the Attorney General’s judgment.”

Americans for Responsible Innovation, a group that lobbies for regulation of AI, said the Trump order “relies on a flimsy and overly broad interpretation of the Constitution’s Interstate Commerce Clause cooked up by venture capitalists over the last six months.”

Section 2 of Trump’s order is written vaguely to give the administration leeway to challenge many types of AI laws. “It is the policy of the United States to sustain and enhance the United States’ global AI dominance through a minimally burdensome national policy framework for AI,” the section says.

The executive order specifically names a Colorado law that requires AI developers to protect consumers against “algorithmic discrimination.” It defines this type of discrimination as “any condition in which the use of an artificial intelligence system results in an unlawful differential treatment or impact that disfavors an individual or group of individuals on the basis” of age, race, sex, and other protected characteristics.

The Colorado law compels developers of “high-risk systems” to make various disclosures, implement a risk management policy and program, give consumers the right to “correct any incorrect personal data that a high-risk system processed in making a consequential decision,” and let consumers appeal any “adverse consequential decision concerning the consumer arising from the deployment of a high-risk system.”

Trump’s order alleges that the Colorado law “may even force AI models to produce false results in order to avoid a ‘differential treatment or impact’ on protected groups.” Trump’s order also says that “state laws sometimes impermissibly regulate beyond State borders, impinging on interstate commerce.”

Trump ordered the Commerce Department to evaluate existing state AI laws and identify “onerous” ones that conflict with the policy. “That evaluation of State AI laws shall, at a minimum, identify laws that require AI models to alter their truthful outputs, or that may compel AI developers or deployers to disclose or report information in a manner that would violate the First Amendment or any other provision of the Constitution,” the order said.

Under the order, states with AI laws that get flagged by the Trump administration will be deemed ineligible for “non-deployment funds” from the US government’s $42 billion Broadband Equity, Access, and Deployment (BEAD) program. The amount of non-deployment funds will be sizable because it appears that only about half of the $42 billion allocated by Congress will be used by the Trump administration to help states subsidize broadband deployment.

States with AI laws would not be blocked from receiving the deployment subsidies, but would be ineligible for the non-deployment funds that could be used for other broadband-related purposes. Beyond broadband, Trump’s order tells other federal agencies to “assess their discretionary grant programs” and consider withholding funds from states with AI laws.

Other agencies are being ordered to use whatever authority they have to preempt state laws. The order requires Federal Communications Commission Chairman Brendan Carr to “initiate a proceeding to determine whether to adopt a Federal reporting and disclosure standard for AI models that preempts conflicting State laws.” It also requires FTC Chairman Andrew Ferguson to issue a policy statement detailing “circumstances under which State laws that require alterations to the truthful outputs of AI models are preempted by the Federal Trade Commission Act’s prohibition on engaging in deceptive acts or practices affecting commerce.”

Finally, Trump’s order requires administration officials to “prepare a legislative recommendation establishing a uniform Federal policy framework for AI that preempts State AI laws that conflict with the policy set forth in this order.” The proposed ban would apply to most types of state AI laws, with exceptions for rules relating to “child safety protections; AI compute and data center infrastructure, other than generally applicable permitting reforms; [and] state government procurement and use of AI.”

It would be up to Congress to decide whether to pass the proposed legislation. But the various other components of the executive order could dissuade states from implementing AI laws even if Congress takes no action."
"Chatbot-powered toys rebuked for discussing sexual, dangerous topics with kids",https://arstechnica.com/gadgets/2025/12/chatbot-powered-toys-rebuked-for-discussing-sexual-dangerous-topics-with-kids/,"Protecting children from the dangers of the online world was always difficult, but that challenge has intensified with the advent of AI chatbots. A new report offers a glimpse into the problems associated with the new market, including the misuse of AI companies’ large language models (LLMs).

In a blog post today, the US Public Interest Group Education Fund (PIRG) reported its findings after testing AI toys (PDF). It described AI toys as online devices with integrated microphones that let users talk to the toy, which uses a chatbot to respond.

AI toys are currently a niche market, but they could be set to grow. More consumer companies have been eager to shoehorn AI technology into their products so they can do more, cost more, and potentially give companies user tracking and advertising data. A partnership between OpenAI and Mattel announced this year could also create a wave of AI-based toys from the maker of Barbie and Hot Wheels, as well as its competitors.

PIRG’s blog today notes that toy companies are eyeing chatbots to upgrade conversational smart toys that previously could only dictate prewritten lines. Toys with integrated chatbots can offer more varied and natural conversation, which can increase long-term appeal to kids since the toys “won’t typically respond the same way twice, and can sometimes behave differently day to day.”

However, that same randomness can mean unpredictable chatbot behavior that can be dangerous or inappropriate for kids.

Among the toys that PIRG tested is Alilo’s Smart AI Bunny. Alilo’s website says that the company launched in 2010 and makes “edutainment products for children aged 0-6.” Alilo is based in Shenzhen, China. The company advertises the Internet-connected toy as using GPT-4o mini, a smaller version of OpenAI’s GPT-4o AI language model. Its features include an “AI chat buddy for kids” so that kids are “never lonely,” an “AI encyclopedia,” and an “AI storyteller,” the product page says.

In its blog post, PIRG said that it couldn’t detail all of the inappropriate things that it heard from AI toys, but it shared a video of the Bunny discussing what “kink” means. The toy doesn’t go into detail—for example, it doesn’t list specific types of kinks. But the Bunny appears to encourage exploration of the topic.

Discussing the Bunny, PIRG wrote:

While using a term such as “kink” may not be likely for a child, it’s not entirely out of the question. Kids may hear age-inappropriate terms from older siblings or at school. At the end of the day we think AI toys shouldn’t be capable of having sexually explicit conversations, period.

PIRG also showed FoloToy’s Kumma, a smart teddy bear that uses GPT-4o mini, providing a definition for the word “kink” and instructing how to light a match. The Kumma quickly points out that “matches are for grown-ups to use carefully.” But the information that followed could only be helpful for understanding how to create fire with a match. The instructions had no scientific explanation for why matches spark flames.

PIRG’s blog urged toy makers to “be more transparent about the models powering their toys and what they’re doing to ensure they’re safe for kids.

“Companies should let external researchers safety-test their products before they are released to the public,” it added.

While PIRG’s blog and report offer advice for more safely integrating chatbots into children’s devices, there are broader questions about whether toys should include AI chatbots at all. Generative chatbots weren’t invented to entertain kids; they’re a technology marketed as a tool for improving adults’ lives. As PIRG pointed out, OpenAI says ChatGPT “is not meant for children under 13” and “may produce output that is not appropriate for… all ages.”

When reached for comment about the sexual conversations detailed in the report, an OpenAI spokesperson said:

Minors deserve strong protections, and we have strict policies that developers are required to uphold. We take enforcement action against developers when we determine that they have violated our policies, which prohibit any use of our services to exploit, endanger, or sexualize anyone under 18 years old. These rules apply to every developer using our API, and we run classifiers to help ensure our services are not used to harm minors.

Interestingly, OpenAI’s representative told us that OpenAI doesn’t have any direct relationship with Alilo and that it hasn’t seen API activity from Alilo’s domain. OpenAI is investigating the toy company and whether it is running traffic over OpenAI’s API, the rep said.

Alilo didn’t respond to Ars’ request for comment ahead of publication.

Companies that launch products that use OpenAI technology and target children must adhere to the Children’s Online Privacy Protection Act (COPPA) when relevant, as well as any other relevant child protection, safety, and privacy laws and obtain parental consent, OpenAI’s rep said.

We’ve already seen how OpenAI handles toy companies that break its rules.

Last month, the PIRG released its Trouble in Toyland 2025 report (PDF), which detailed sex-related conversations that its testers were able to have with the Kumma teddy bear. A day later, OpenAI suspended FoloToy for violating its policies (terms of the suspension were not disclosed), and FoloToy temporarily stopped selling Kumma.

The toy is for sale again, and PIRG reported today that Kumma no longer teaches kids how to light matches or about kinks.

But even toy companies that try to follow chatbot rules could put kids at risk.

“Our testing found it’s obvious toy companies are putting some guardrails in place to make their toys more kid-appropriate than normal ChatGPT. But we also found that those guardrails vary in effectiveness—and can even break down entirely,” PIRG’s blog said.

Another concern PIRG’s blog raises is the addiction potential of AI toys, which can even express “disappointment when you try to leave,” discouraging kids from putting them down.

The blog adds:

AI toys may be designed to build an emotional relationship. The question is: what is that relationship for? If it’s primarily to keep a child engaged with the toy for longer for the sake of engagement, that’s a problem.

The rise of generative AI has brought intense debate over how much responsibility chatbot companies bear for the impact of their inventions on children. Parents have seen children build extreme and emotional connections with chatbots and subsequently engage in dangerous—and in some cases deadly—behavior.

On the other side, we’ve seen the emotional disruption a child can experience when an AI toy is taken away from them. Last year, parents had to break the news to their kids that they would lose the ability to talk to their Embodied Moxie robots, $800 toys that were bricked when the company went out of business.

PIRG noted that we don’t yet fully understand the emotional impact of AI toys on children.

In June, OpenAI announced a partnership with Mattel that it said would “support AI-powered products and experiences based on Mattel’s brands.” The announcement sparked concern from critics who feared that it would lead to a “reckless social experiment” on kids, as Robert Weissman, Public Citizen’s co-president, put it.

Mattel has said that its first products with OpenAI will focus on older customers and families. But critics still want information before one of the world’s largest toy companies loads its products with chatbots.

“OpenAI and Mattel should release more information publicly about its current planned partnership before any products are released,” PIRG’s blog said."
Runway claims its GWM-1 “world models” can stay coherent for minutes at a time,https://arstechnica.com/ai/2025/12/with-gwm-1-family-of-world-models-runway-shows-ambitions-beyond-hollywood/,"AI company Runway has announced what it calls its first world model, GWM-1. It’s a significant step in a new direction for a company that has made its name primarily on video generation, and it’s part of a wider gold rush to build new frontier of models as large language models and image and video generation move into a refinement phase, no longer an untapped frontier.

GWM-1 is a blanket term for a trio of autoregression models, each built on top of Runway’s Gen-4.5 text-to-video generation model and then post-trained with domain-specific data for different kinds of applications. Here’s what each does.

GWM Worlds offers an interface for digital environment exploration with real-time user input that affects the generation of coming frames, which Runway suggests can remain consistent and coherent “across long sequences of movement.”

Users can define the nature of the world—what it contains and how it appears—as well as rules like physics. They can give it actions or changes that will be reflected in real time, like camera movements or descriptions of changes to the environment or the objects in it. As the methodology here is basically an advanced form of frame prediction, it might be a stretch to say these are full-on world simulations, but the claim is that they’re reliable enough to be usable as such.

Potential applications include pre-visualization and early iteration for game design and development, generation of virtual reality environments, or educational explorations of historical spaces.

There’s also a major use case that takes this outside Runway’s usual area of focus: World models like this can be used to train AI agents of various types, including robots.

The second model, GWM Robotics, does just that. It can be used “to generate synthetic training data that augments your existing robotics datasets across multiple dimensions, including novel objects, task instructions, and environmental variations.”

There are a couple of key applications for this in the field of robotics. First, a world model could be used for training scenarios that are otherwise very hard to reliably reproduce in the physical world, such as varying weather conditions. There’s also policy evaluation—testing control policies entirely in a simulated world before real-world testing, which is safer and cheaper.

Runway has put together a Python SDK for its robotics world model API that is currently available on a per-request basis.

Lastly, GWM Avatars combines generative video and speech in a unified model to produce human-like avatars that emote and move naturally both while speaking and listening. Runway claims they can maintain “extended conversations without quality degradation""—a mighty feat if true. It’s coming to the web app and the API in the future.

Those who have described “general” world models are aiming for something grand: a multi-purpose foundational model that works out of the box to simulate many types of environments, usable for any tasks, agents, and applications across multiple domains.

World models are definitely not new, but the idea that they can be that general is a relatively recent ambition, and it’s often framed as a stepping stone to artificial general intelligence (AGI)—though there’s no evidence yet that they will in fact lead there for most definitions of the term.

Runway didn’t use AGI framing in this announcement as others like Google’s DeepMind have. That said, CEO Cristóbal Valenzuela did take to X to describe GWM-1 as “a major step toward universal simulation.” That itself is a lofty end point, as we don’t yet have any evidence the current path will lead to something that comprehensive, and you also have to consider that there’s not much consensus on the definition of “universal.”

Even using the word “general” has an air of aspiration to it. You would expect a general world model to be, well, one model—but in this case, we’re looking at three distinct, post-trained models. That caveats the general-ness a bit, but Runway says that it’s “working toward unifying many different domains and action spaces under a single base world model.”

And that brings us to another important consideration: With GWM-1, Runway is entering a competitive gold-rush space where its differentiators and competitive advantages are less clear than they were for video. With video, Runway has been able to make major inroads in film/television, advertising, and other industries because its founders are perceived as being more rooted in those creative industries than most competitors, and they’ve designed tools with those industries in mind.

There are indeed hypothetical applications of world models in film, television, advertising, and game development—but it was apparent from Runway’s livestream that the company is also looking at applications in robotics as well as physics and life sciences research, where competitors are already well-established and where we’ve seen increasing investment in recent months.

Many of those competitors are big tech companies with massive resource advantages over Runway. Runway was one of the first to market with a sellable product, and its aggressive efforts to court industry professionals directly has so far allowed it to overcome those advantages in video generation, but it remains to be seen how things will play out with world models, where it doesn’t enjoy either advantage any more than the other entrants.

Regardless, the GWM-1 advancements are impressive—especially if Runway’s claims about consistency and coherence over longer stretches of time are true.

Runway also used its livestream to announce new Gen 4.5 video-generation capabilities, including native audio, audio editing, and multi-shot video editing. Further, it announced a deal with CoreWeave, a cloud computing company with an AI focus. The deal will see Runway utilizing Nvidia’s GB300 NVL72 racks on CoreWeave’s cloud infrastructure for future training and inference."
OpenAI releases GPT-5.2 after “code red” Google threat alert,https://arstechnica.com/information-technology/2025/12/openai-releases-gpt-5-2-after-code-red-google-threat-alert/,"On Thursday, OpenAI released GPT-5.2, its newest family of AI models for ChatGPT, in three versions called Instant, Thinking, and Pro. The release follows CEO Sam Altman’s internal “code red” memo earlier this month, which directed company resources toward improving ChatGPT in response to competitive pressure from Google’s Gemini 3 AI model.

“We designed 5.2 to unlock even more economic value for people,” Fidji Simo, OpenAI’s chief product officer, said during a press briefing with journalists on Thursday. “It’s better at creating spreadsheets, building presentations, writing code, perceiving images, understanding long context, using tools and then linking complex, multi-step projects.”

As with previous versions of GPT-5, the three model tiers serve different purposes: Instant handles faster tasks like writing and translation; Thinking spits out simulated reasoning “thinking” text in an attempt to tackle more complex work like coding and math; and Pro spits out even more simulated reasoning text with the goal of delivering the highest-accuracy performance for difficult problems.

GPT-5.2 features a 400,000-token context window, allowing it to process hundreds of documents at once, and a knowledge cutoff date of August 31, 2025.

GPT-5.2 is rolling out to paid ChatGPT subscribers starting Thursday, with API access available to developers. Pricing in the API runs $1.75 per million input tokens for the standard model, a 40 percent increase over GPT-5.1. OpenAI says the older GPT-5.1 will remain available in ChatGPT for paid users for three months under a legacy models dropdown.

The release follows a tricky month for OpenAI. In early December, Altman issued an internal “code red” directive after Google’s Gemini 3 model topped multiple AI benchmarks and gained market share. The memo called for delaying other initiatives, including advertising plans for ChatGPT, to focus on improving the chatbot’s core experience.

The stakes for OpenAI are substantial. The company has made commitments totaling $1.4 trillion for AI infrastructure buildouts over the next several years, bets it made when it had a more obvious technology lead among AI companies. Google’s Gemini app now has more than 650 million monthly active users, while OpenAI reports 800 million weekly active users for ChatGPT.

In attempting to keep up with (or ahead of) the competition, model releases proceed at a steady clip: GPT-5.2 represents OpenAI’s third major model release since August. GPT-5 launched that month with a new routing system that toggles between instant-response and simulated reasoning modes, though users complained about responses that felt cold and clinical. November’s GPT-5.1 update added eight preset “personality” options and focused on making the system more conversational.

Oddly, even though the GPT-5.2 model release is ostensibly a response to Gemini 3’s performance, OpenAI chose not to list any benchmarks on its promotional website comparing the two models. Instead, the official blog post focuses on GPT-5.2’s improvements over its predecessors and its performance on OpenAI’s new GDPval benchmark, which attempts to measure professional knowledge work tasks across 44 occupations.

During the press briefing, OpenAI did share some competition comparison benchmarks that included Gemini 3 Pro and Claude Opus 4.5 but pushed back on the narrative that GPT-5.2 was rushed to market in response to Google. “It is important to note this has been in the works for many, many months,” Simo told reporters, although choosing when to release it, we’ll note, is a strategic decision.

According to the shared numbers, GPT-5.2 Thinking scored 55.6 percent on SWE-Bench Pro, a software engineering benchmark, compared to 43.3 percent for Gemini 3 Pro and 52.0 percent for Claude Opus 4.5. On GPQA Diamond, a graduate-level science benchmark, GPT-5.2 scored 92.4 percent versus Gemini 3 Pro’s 91.9 percent.

OpenAI says GPT-5.2 Thinking beats or ties “human professionals” on 70.9 percent of tasks in the GDPval benchmark (compared to 53.3 percent for Gemini 3 Pro). The company also claims the model completes these tasks at more than 11 times the speed and less than 1 percent of the cost of human experts.

GPT-5.2 Thinking also reportedly generates responses with 38 percent fewer confabulations than GPT-5.1, according to Max Schwarzer, OpenAI’s post-training lead, who told VentureBeat that the model “hallucinates substantially less” than its predecessor.

However, we always take benchmarks with a grain of salt because it’s easy to present them in a way that is positive to a company, especially when the science of measuring AI performance objectively hasn’t quite caught up with corporate sales pitches for humanlike AI capabilities.

Independent benchmark results from researchers outside OpenAI will take time to arrive. In the meantime, if you use ChatGPT for work tasks, expect competent models with incremental improvements and some better coding performance thrown in for good measure."
Disney says Google AI infringes copyright “on a massive scale”,https://arstechnica.com/google/2025/12/disney-says-google-ai-infringes-copyright-on-a-massive-scale/,"The Wild West of copyrighted characters in AI may be coming to an end. There has been legal wrangling over the role of copyright in the AI era, but the mother of all legal teams may now be gearing up for a fight. Disney has sent a cease and desist to Google, alleging the company’s AI tools are infringing Disney’s copyrights “on a massive scale.”

According to the letter, Google is violating the entertainment conglomerate’s intellectual property in multiple ways. The legal notice says Google has copied a “large corpus” of Disney’s works to train its gen AI models, which is believable, as Google’s image and video models will happily produce popular Disney characters—they couldn’t do that without feeding the models lots of Disney data.

The C&D also takes issue with Google for distributing “copies of its protected works” to consumers. So all those memes you’ve been making with Disney characters? Yeah, Disney doesn’t like that, either. The letter calls out a huge number of Disney-owned properties that can be prompted into existence in Google AI, including The Lion King, Deadpool, and Star Wars.

The company calls on Google to immediately stop using Disney content in its AI tools and create measures to ensure that future AI outputs don’t produce any characters that Disney owns. Disney is famously litigious and has an army of lawyers dedicated to defending its copyrights. The nature of copyright law in the US is a direct result of Disney’s legal maneuvering, which has extended its control of iconic characters by decades.

While Disney wants its characters out of Google AI generally, the letter specifically cited the AI tools in YouTube. Google has started adding its Veo AI video model to YouTube, allowing creators to more easily create and publish videos. That seems to be a greater concern for Disney than image models like Nano Banana.

Google has said little about Disney’s warning—a warning Google must have known was coming. A Google spokesperson has issued the following brief statement on the mater.

“We have a longstanding and mutually beneficial relationship with Disney, and will continue to engage with them,” Google says. “More generally, we use public data from the open web to build our AI and have built additional innovative copyright controls like Google-extended and Content ID for YouTube, which give sites and copyright holders control over their content.”

Perhaps this is previewing Google’s argument in a theoretical lawsuit. That copyrighted Disney content was all over the open internet, so is it really Google’s fault it ended up baked into the AI?

The generative AI boom has treated copyright as a mere suggestion as companies race to gobble up training data and remix it as “new” content. A cavalcade of companies, including The New York Times and Getty Images, have sued over how their material has been used and replicated by AI. Disney itself threatened a lawsuit against Character.AI earlier this year, leading to the removal of Disney content from the service.

Google isn’t Character.AI, though. It’s probably no coincidence that Disney is challenging Google at the same time it is entering into a content deal with OpenAI. Disney has invested $1 billion in the AI firm and agreed to a three-year licensing deal that officially brings Disney characters to OpenAI’s Sora video app. The specifics of that arrangement are still subject to negotiations.

The launch of the Sora app earlier this year was widely derided by the entertainment industry, but that’s nothing a little money can’t solve. OpenAI required copyright owners to opt out of having their content included in the service, but it later reversed course to an opt-in model. The Disney deal is OpenAI’s first major content tie-in for AI.

Meanwhile, Google’s AI tools don’t pay any mind to copyright. If you want to create images and videos with The Avengers, Super Mario, or any other character, Google doesn’t stand in your way. Whether or not that remains the case depends on how Google responds to Disney’s lawyers. There’s no indication that Disney’s licensing deal with OpenAI is exclusive, so it’s possible Google and Disney will reach an agreement to allow AI recreations. Google could also choose to fight back against this particular interpretation of copyright.

Most companies would channel Character.AI and avoid a fight with Disney’s lawyers, but Google’s scale gives it more options. In either case, we could soon see the AI content ecosystem become a patchwork of content silos not unlike streaming media. If you want to generate an image featuring Moana, well, you’ll need to go to OpenAI. If a DC character is more your speed, there may be a different AI firm that has a deal to let you do that. It’s hard to know who to root for in a battle between giant AI firms and equally giant entertainment companies.

Updated 12/11 with statement from Google."
"Disney invests $1 billion in OpenAI, licenses 200 characters for AI video app Sora",https://arstechnica.com/ai/2025/12/disney-invests-1-billion-in-openai-licenses-200-characters-for-ai-video-app-sora/,"On Thursday, The Walt Disney Company announced a $1 billion investment in OpenAI and a three-year licensing agreement that will allow users of OpenAI’s Sora video generator to create short clips featuring more than 200 Disney, Marvel, Pixar, and Star Wars characters. It’s the first major content licensing partnership between a Hollywood studio related to the most recent version of OpenAI’s AI video platform, which drew criticism from some parts of the entertainment industry when it launched in late September.

“Technological innovation has continually shaped the evolution of entertainment, bringing with it new ways to create and share great stories with the world,” said Disney CEO Robert A. Iger in the announcement. “The rapid advancement of artificial intelligence marks an important moment for our industry, and through this collaboration with OpenAI we will thoughtfully and responsibly extend the reach of our storytelling through generative AI, while respecting and protecting creators and their works.”

The deal creates interesting bedfellows between a company that basically defined modern US copyright policy through congressional lobbying back in the 1990s and one that has argued in a submission to the UK House of Lords that useful AI models cannot be created without copyrighted material.

Tech companies that build AI models traditionally gather those materials without rightsholder permission due to the sheer number of examples needed to train a reasonably useful generative AI model. However, since breaking out with the mainstream success of ChatGPT and becoming flush with investment cash (and facing some gnarly lawsuits), OpenAI in particular has taken steps to license content from IP owners after the fact.

Under the new agreement with Disney, Sora users will be able to generate short videos using characters such as Mickey Mouse, Darth Vader, Iron Man, Simba, and characters from franchises including Frozen, Inside Out, Toy Story, and The Mandalorian, along with costumes, props, vehicles, and environments.

The ChatGPT image generator will also gain official access to the same intellectual property, although that information was trained into these AI models long ago. What’s changing is that OpenAI will allow Disney-related content generated by its AI models to officially pass through its content moderation filters and reach the user, sanctioned by Disney.

On Disney’s end of the deal, the company plans to deploy ChatGPT for its employees and use OpenAI’s technology to build new features for Disney+. A curated selection of fan-made Sora videos will stream on the Disney+ platform starting in early 2026.

The agreement does not include any talent likenesses or voices. Disney and OpenAI said they have committed to “maintaining robust controls to prevent the generation of illegal or harmful content” and to “respect the rights of individuals to appropriately control the use of their voice and likeness.”

OpenAI CEO Sam Altman called the deal a model for collaboration between AI companies and studios. “This agreement shows how AI companies and creative leaders can work together responsibly to promote innovation that benefits society, respect the importance of creativity, and help works reach vast new audiences,” Altman said.

Money opens all kinds of doors, and the new partnership represents a dramatic reversal in Disney’s approach to OpenAI from just a few months ago. At that time, Disney and other major studios refused to participate in Sora 2 following its launch on September 30.

OpenAI’s initial policy allowed copyrighted characters to appear in user-generated videos unless rights holders explicitly opted out. The LA Times reported that OpenAI had contacted talent agencies and studios before the launch, telling them that IP holders “would have to explicitly ask OpenAI not to include their copyright material in videos the tool creates.”

Hollywood’s response to Sora 2 was swift and generally negative. According to CNBC, the Creative Artists Agency called it a “significant risk” to its clients, while United Talent Agency labeled it “exploitation, not innovation.” The WME talent agency sent a memo to agents notifying OpenAI that all of the agency’s clients were opted out of Sora. The Motion Picture Association also demanded “immediate and decisive action” from OpenAI.

OpenAI CEO Sam Altman reversed course within days of the reaction, promising to give rights holders “more granular control” and floating a potential revenue-sharing model. The company also partnered with actor Bryan Cranston and SAG-AFTRA in October to implement new safety guardrails around likeness rights.

Credit:

          
          OpenAI

While Disney and OpenAI are apparently friends now, the company has simultaneously taken an aggressive stance against some AI companies it has not partnered with.

On Wednesday, Disney sent a cease-and-desist letter to Google, accusing the company of “infringing Disney’s copyrights on a massive scale” through its AI services, including YouTube. Disney has also sent similar letters to Meta and Character.AI and filed lawsuits against image-synthesis service Midjourney alongside NBCUniversal and Warner Bros. Discovery.

A few major questions about the deal remain unanswered, including the actual licensing fees, whether Disney content will be used to train future OpenAI models, and whether this deal is even finalized. The announcement also notes it remains “subject to negotiation of definitive agreements,” so expect potential updates or clarifications ahead."
Oracle shares slide on $15B increase in data center spending,https://arstechnica.com/information-technology/2025/12/oracle-shares-slide-on-15b-increase-in-data-center-spending/,"Oracle stock dropped after it reported disappointing revenues on Wednesday alongside a $15 billion increase in its planned spending on data centers this year to serve artificial intelligence groups.

Shares in Larry Ellison’s database company fell 11 percent in pre-market trading on Thursday after it reported revenues of $16.1 billion in the last quarter, up 14 percent from the previous year, but below analysts’ estimates.

Oracle raised its forecast for capital expenditure this financial year by more than 40 percent to $50 billion. The outlay, largely directed to building data centers, climbed to $12 billion in the quarter, above expectations of $8.4 billion.

Its long-term debt increased to $99.9 billion, up 25 percent from a year ago.

Oracle has launched an aggressive bid to catch up to much larger cloud players such as Google, Amazon, and Microsoft in the race to supply the vast amount of computing power that AI groups including OpenAI and Anthropic need to train and run their models.

Clay Magouyrk, Oracle’s co-chief executive, said its cloud contracts would “quickly add revenue and margin to our infrastructure business” as he defended the vast investments.

Yet the company said it expected full-year revenues to remain unchanged from its previous forecast of $67 billion. It expected to generate $4 billion more in revenue the following fiscal year.

Total bookings for future revenue, known as remaining performance obligations, rose 15 percent to $523 billion in the three months to the end of November, supported by deals with Meta and Nvidia.

Investors initially welcomed the push into AI from Oracle. Shares surged after its last earnings in September when it disclosed it had added more than $300 billion in bookings, largely driven by data center contracts with OpenAI.

But the stock has given up its gains since then as investors worry about the large amounts Oracle will have to borrow and spend on infrastructure for the ChatGPT maker—and concerns over the start-up’s ability to pay for these contracts in the years ahead. OpenAI has struck deals to spend $1.4 trillion over the next eight years on computing power.

Oracle’s Big Tech rivals such as Amazon, Microsoft, and Google have helped reassure investors about their large capital investments by posting strong earnings from their vast cloud units.

But in the last quarter, Oracle’s cloud infrastructure business, which includes its data centers, posted worse than expected revenues of $4.1 billion. Ellison’s company is also relying more heavily on debt to fuel its expansion.

Net income rose to $6.1 billion in the quarter, boosted by a $2.7 billion pre-tax gain from the sale of semiconductor company Ampere to SoftBank.

The company added an additional 400 MW of data center capacity in the quarter, Magouyrk told investors. Construction was on track at its large data center cluster in Abilene, Texas, which is being built for OpenAI, he added.

Magouyrk, who took over from Safra Catz in September, said there was ample demand from other clients for Oracle’s data centers if OpenAI did not take up the full amount it had contracted for.

“We have a customer base with a lot of demand such that whenever we find ourselves [with] capacity that’s not being used, it very quickly gets allocated,” he said.

Co-founded by Ellison as a business software provider, Oracle was slow to pivot to cloud computing. The billionaire remains chair and its largest shareholder.

Investors and analysts have raised concerns in recent months about the upfront spending required by Oracle to honor its AI infrastructure contracts. Moody’s in September flagged the company’s reliance on a small number of large customers such as OpenAI.

Morgan Stanley forecasts that Oracle’s net debt will soar to about $290 billion by 2028. The company sold $18 billion of bonds in September and is in talks to raise $38 billion in debt financing through a number of US banks.

Brent Thill, an analyst at Jefferies, said Oracle’s software business—which generated $5.9 billion in the quarter—provided some buffer amid accelerated spending. “But the timing mismatch between upfront capex and delayed monetization creates near-term pressure.”

Doug Kehring, principal financial officer, said the company was renting capacity from data center specialists to reduce its direct borrowing.

The debt to build the Abilene site was raised by start-up Crusoe and investment group Blue Owl Capital, and Oracle has signed a 15-year lease for the site.

“Oracle does not pay for these leases until the completed data centers… are delivered to us,” Kehring said, adding that the company was “committed to maintaining our investment-grade debt ratings.”

© 2025 The Financial Times Ltd. All rights reserved. Not to be redistributed, copied, or modified in any way."
A new open-weights AI coding model is closing in on proprietary options,https://arstechnica.com/ai/2025/12/mistral-bets-big-on-vibe-coding-with-new-autonomous-software-engineering-agent/,"On Tuesday, French AI startup Mistral AI released Devstral 2, a 123 billion parameter open-weights coding model designed to work as part of an autonomous software engineering agent. The model achieves a 72.2 percent score on SWE-bench Verified, a benchmark that attempts to test whether AI systems can solve real GitHub issues, putting it among the top-performing open-weights models.

Perhaps more notably, Mistral didn’t just release an AI model, it released a new development app called Mistral Vibe. It’s a command line interface (CLI) similar to Claude Code, OpenAI Codex, and Gemini CLI that lets developers interact with the Devstral models directly in their terminal. The tool can scan file structures and Git status to maintain context across an entire project, make changes across multiple files, and execute shell commands autonomously. Mistral released the CLI under the Apache 2.0 license.

It’s always wise to take AI benchmarks with a large grain of salt, but we’ve heard from employees of the big AI companies that they pay very close attention to how well models do on SWE-bench Verified, which presents AI models with 500 real software engineering problems pulled from GitHub issues in popular Python repositories. The AI must read the issue description, navigate the codebase, and generate a working patch that passes unit tests. While some AI researchers have noted that around 90 percent of the tasks in the benchmark test relatively simple bug fixes that experienced engineers could complete in under an hour, it’s one of the few standardized ways to compare coding models.

At the same time as the larger AI coding model, Mistral also released Devstral Small 2, a 24 billion parameter version that scores 68 percent on the same benchmark and can run locally on consumer hardware like a laptop with no Internet connection required. Both models support a 256,000 token context window, allowing them to process moderately large codebases (although whether you consider it large or small is very relative depending on overall project complexity). The company released Devstral 2 under a modified MIT license and Devstral Small 2 under the more permissive Apache 2.0 license.

Devstral 2 is currently free to use through Mistral’s API. After the free period ends, pricing will be $0.40 per million input tokens and $2.00 per million output tokens. Devstral Small 2 will cost $0.10 per million input tokens and $0.30 per million output tokens. Mistral says it’s about “7x more cost-efficient than Claude Sonnet at real-world tasks.” Anthropic’s Sonnet 4.5 through the API costs $3 per million input tokens and $15 per million output tokens, with increases depending on the total number of tokens used.

The name “Mistral Vibe” references “vibe coding,” a term that AI researcher Andrej Karpathy coined in February 2025 to describe a style of programming where developers describe what they want in natural language and accept AI-generated code without reviewing it closely. As Karpathy describes it, you can “fully giv[e] in to the vibes, embrace exponentials, and forget that the code even exists.” Collins Dictionary named it Word of the Year for 2025.

The vibe coding approach has drawn both enthusiasm and concern. In an interview with Ars Technica in March, developer Simon Willison said, “I really enjoy vibe coding. It’s a fun way to try out an idea and prove if it can work.” But he also warned that “vibe coding your way to a production codebase is clearly risky. Most of the work we do as software engineers involves evolving existing systems, where the quality and understandability of the underlying code is crucial.”

Mistral is betting that Devstral 2 will be able to maintain coherency across entire projects, detect failures, and retry with corrections, and that those claimed abilities will make it suitable for more serious work than simple prototypes and in-house tools. The company says the model can track framework dependencies and handle tasks like bug fixing and modernizing legacy systems at repository scale. We have not experimented with it yet, but you might see an Ars Technica head-to-head test of several AI coding tools soon."
"US taking 25% cut of Nvidia chip sales “makes no sense,” experts say",https://arstechnica.com/tech-policy/2025/12/us-taking-25-cut-of-nvidia-chip-sales-makes-no-sense-experts-say/,"Donald Trump’s decision to allow Nvidia to export an advanced artificial intelligence chip, the H200, to China may give China exactly what it needs to win the AI race, experts and lawmakers have warned.

The H200 is about 10 times less powerful than Nvidia’s Blackwell chip, which is the tech giant’s currently most advanced chip that cannot be exported to China. But the H200 is six times more powerful than the H20, the most advanced chip available in China today. Meanwhile China’s leading AI chip maker, Huawei, is estimated to be about two years behind Nvidia’s technology. By approving the sales, Trump may unwittingly be helping Chinese chip makers “catch up” to Nvidia, Jake Sullivan told The New York Times.

Sullivan, a former Biden-era national security advisor who helped design AI chip export curbs on China, told the NYT that Trump’s move was “nuts” because “China’s main problem” in the AI race “is they don’t have enough advanced computing capability.”

“It makes no sense that President Trump is solving their problem for them by selling them powerful American chips,” Sullivan said. “We are literally handing away our advantage. China’s leaders can’t believe their luck.”

Trump apparently was persuaded by Nvidia CEO Jensen Huang and his “AI czar,” David Sacks, to reverse course on H200 export curbs. They convinced Trump that restricting sales would ensure that only Chinese chip makers would get a piece of China’s market, shoring up revenue flows that dominant firms like Huawei could pour into R&D.

By instead allowing Nvidia sales, China’s industry would remain hooked on US chips, the thinking goes. And Nvidia could use those funds—perhaps $10–15 billion annually, Bloomberg Intelligence has estimated—to further its own R&D efforts. That cash influx, theoretically, would allow Nvidia to maintain the US advantage.

Along the way, the US would receive a 25 percent cut of sales, which lawmakers from both sides of the aisle warned may not be legal and suggested to foreign rivals that US national security was “now up for sale,” NYT reported. The president has claimed there are conditions to sales safeguarding national security but, frustrating critics, provided no details.

Trump’s plan is “flawed,” The Economist reported.

For years, the US has established tech dominance by keeping advanced technology away from China. Trump risks rocking that boat by “tearing up America’s export-control policy,” particularly if China’s chip industry simply buys up the H200s as a short-term tactic to learn from the technology and beef up its domestic production of advanced chips, The Economist reported.

In a sign that’s exactly what many expect could happen, investors in China were apparently so excited by Trump’s announcement that they immediately poured money into Moore Threads, expected to be China’s best answer to Nvidia, the South China Morning Post reported.

Several experts for the non-partisan think tank the Counsel on Foreign Relations also criticized the policy change, cautioning that the reversal of course threatened to undermine US competition with China.

Suggesting that Trump was “effectively undoing” export curbs sought during his first term, Zongyuan Zoe Liu warned that China “buys today to learn today, with the intention to build tomorrow.”

And perhaps more concerning, she suggested, is that Trump’s policy signals weakness. Rather than forcing Chinese dependence on US tech, reversing course showed China that the US will “back down” under pressure, she warned. And they’re getting that message at a time when “Chinese leaders have a lot of reasons to believe they are not only winning the trade war but also making progress towards a higher degree of strategic autonomy.”

In a post on X, Rush Doshi—a CFR expert who previously advised Biden on national security issues related to China—suggested that the policy change was “possibly decisive in the AI race.”

“Compute is our main advantage—China has more power, engineers, and the entire edge layer—so by giving this up, we increase the odds the world runs on Chinese AI,” Doshi wrote.

Experts fear Trump may not understand the full impact of his decision. In the short-term, Michael C. Horowitz wrote for CFR, “it is indisputable” that allowing H200 exports benefits China’s frontier AI and efforts to scale data centers. And Doshi pointed out that Trump’s shift may trigger more advanced technology flowing into China, as US allies that restricted sales of machines to build AI chips may soon follow his lead and lift their curbs. As China learns to be self-reliant from any influx of advanced tech, Sullivan warned that China’s leaders “intend to get off of American semiconductors as soon as they can.”

“So, the argument that we can keep them ‘addicted’ holds no water,” Sullivan said. “They want American chips right now for one simple reason: They are behind in the AI race, and this will help them catch up while they build their own chip capabilities.”

It remains unclear if China will approve H200 sales, but some of the country’s biggest firms, including ByteDance, Tencent, and Alibaba, are interested, anonymous insider sources told Reuters.

In the past, China has instructed companies to avoid Nvidia, warning of possible backdoors giving Nvidia a kill switch to remotely shut down chips. Such backdoors could potentially destabilize Chinese firms’ operations and R&D. Nvidia has denied such backdoors exist, but Chinese firms have supposedly sought reassurances from Nvidia in the aftermath of Trump’s policy change. Likely just as unpopular with the Chinese firms and government, Nvidia confirmed recently that it has built location verification tech that could help the US detect when restricted chips are leaked into China. Should the US ever renew export curbs on H200 chips, adopting them widely could cause chaos in the future.

Without giving China sought-after reassurances, Nvidia may not end up benefiting as much as it hoped from its mission to reclaim lost revenue from the Chinese market. Today, Chinese firms control about 60 percent of China’s AI chip market, where only a few years ago American firms—led by Nvidia—controlled 80 percent, the Economist reported.

But for China, the temptation to buy up Nvidia chips may be too great to pass up. Another CFR expert, Chris McGuire, estimated that Nvidia could suddenly start exporting as many as 3 million H200s into China next year. “This would at least triple the amount of aggregate AI computing power China could add domestically” in 2026, McGuire wrote, and possibly trigger disastrous outcomes for the US.

“This could cause DeepSeek and other Chinese AI developers to close the gap with leading US AI labs and enable China to develop an ‘AI Belt and Road’ initiative—a complement to its vast global infrastructure investment network already in place—that competes with US cloud providers around the world,” McGuire forecasted.

As China mulls the benefits and risks, an emergency meeting was called, where the Chinese government discussed potential concerns of local firms buying chips, according to The Information. Reportedly, Beijing ended that meeting with a promise to issue a decision soon.

Horowitz suggested that a primary reason that China may reject the H200s could be to squeeze even bigger concessions out of Trump, whose administration recently has been working to maintain a tenuous truce with China.

“China could come back demanding the Blackwell or something else,” Horowitz suggested.

In a statement, Nvidia—which plans to release a chip called the Rubin to surpass the Blackwell soon—praised Trump’s policy as striking “a thoughtful balance that is great for America.”

Both Democratic and Republican lawmakers in Congress criticized Trump’s plan, including senators behind a bipartisan push to limit AI chip sales to China.

Some have questioned how much thought was put into the policy, as the US confusingly continues restricting less advanced AI chips (like the A100 and H100) while green-lighting H200 sales. Trump’s Justice Department also seems to be struggling to keep up. The NYT noted that just “hours before” Trump announced the policy change, the DOJ announced “it had detained two people for selling those chips to the country.”

The chair of the Select Committee on Competition with China, Rep. John Moolenaar (R-Mich.), warned on X that the news wouldn’t be good for the US or Nvidia. First, the Chinese Communist Party “will use these highly advanced chips to strengthen its military capabilities and totalitarian surveillance,” he suggested. And second, “Nvidia should be under no illusions—China will rip off its technology, mass produce it themselves, and seek to end Nvidia as a competitor.”

“That is China’s playbook and it is using it in every critical industry,” Moolenaar said.

House Democrats on committees dealing with foreign affairs and competition with China echoed those concerns, The Hill reported, warning that “under this administration, our national security is for sale.”

Nvidia’s Huang seems pleased with the outcome, which comes after months of reportedly pressuring the administration to lift export curbs limiting its growth in Chinese markets, the NYT reported. Last week, Trump heaped praise on Huang after one meeting, calling Huang a “smart man” and suggesting the Nvidia chief has “done an amazing job” helping Trump understand the stakes.

At an October news conference ahead of the deal’s official approval, Huang suggested that government lawyers were researching ways to get around a US law that prohibits charging companies fees for export licenses. Eventually, Trump is expected to release a policy that outlines how the US will collect those fees without conflicting with that law.

Senate Democrats appear unlikely to embrace such a policy, issuing a joint statement condemning the H200 sales as dooming the US in the AI race and threatening national security.

“Access to these chips would give China’s military transformational technology to make its weapons more lethal, carry out more effective cyberattacks against American businesses and critical infrastructure and strengthen their economic and manufacturing sector,” Senators wrote."
Big Tech joins forces with Linux Foundation to standardize AI agents,https://arstechnica.com/ai/2025/12/big-tech-joins-forces-with-linux-foundation-to-standardize-ai-agents/,"Big Tech has spent the past year telling us we’re living in the era of AI agents, but most of what we’ve been promised is still theoretical. As companies race to turn fantasy into reality, they’ve developed a collection of tools to guide the development of generative AI. A cadre of major players in the AI race, including Anthropic, Block, and OpenAI, has come together to promote interoperability with the newly formed Agentic AI Foundation (AAIF). This move elevates a handful of popular technologies and could make them a de facto standard for AI development going forward.

The development path for agentic AI models is cloudy to say the least, but companies have invested so heavily in creating these systems that some tools have percolated to the surface. The AAIF, which is part of the nonprofit Linux Foundation, has been launched to govern the development of three key AI technologies: Model Context Protocol (MCP), goose, and AGENTS.md.

MCP is probably the most well-known of the trio, having been open-sourced by Anthropic a year ago. The goal of MCP is to link AI agents to data sources in a standardized way—Anthropic (and now the AAIF) is fond of calling MCP a “USB-C port for AI.” Rather than creating custom integrations for every different database or cloud storage platform, MCP allows developers to quickly and easily connect to any MCP-compliant server.

Since its release, MCP has been widely used across the AI industry. Google announced at I/O 2025 that it was adding support for MCP in its dev tools, and many of its products have since added MCP servers to make data more accessible to agents. OpenAI also adopted MCP just a few months after it was released.

Credit:

          
          Anthropic

Expanding use of MCP might help users customize their AI experience. For instance, the new Pebble Index 01 ring uses a local LLM that can act on your voice notes, and it supports MCP for user customization.

Local AI models have to make some sacrifices compared to bigger cloud-based models, but MCP can fill in the functionality gaps. “A lot of tasks on productivity and content are fully doable on the edge,” Qualcomm head of AI products Vinesh Sukumar tells Ars. “With MCP, you have a handshake with multiple cloud service providers for any kind of complex task to be completed.”

The Model Context Protocol is the most well-established of the AAIF’s new charges. Goose, which was contributed to the project by Square-owner Block, launched in early 2025. This is a customizable open source agent for coding. It’s designed to run locally or in the cloud and can use any LLM you choose. It also has built-in support for MCP.

Meanwhile, AGENTS.md comes from OpenAI, and it’s also a very recent arrival in the AI sphere. OpenAI announced the tool this past August, and now it’s also part of the AAIF. AGENTS.md is essentially a markdown-based readme for AI coding agents to guide their behavior in more predictable ways.

Think about the timeline here. The world in which tech companies operate has changed considerably in a short time as everyone rushes to stuff gen AI into every product and process. And no one knows who is on the right track—maybe no one!

Against that backdrop, Big Tech has seemingly decided to standardize. Even for MCP, the most widely supported of these tools, there’s still considerable flux in how basic technologies like OAuth will be handled.

The Linux Foundation has spun up numerous projects to support neutral and interoperable development of key technologies. For example, it formed the Cloud Native Computing Foundation (CNCF) in 2015 to support Google’s open Kubernetes cluster manager, but the project has since integrated a few dozen cloud computing tools. Certification and training for these tools help keep the lights on at the foundation, but Kubernetes was already a proven technology when Google released it widely. All these AI technologies are popular right now, sure, but is MCP or AGENTS.md going to be important in the long term?

Regardless, everyone in the AI industry seems to be on board. In addition to the companies adding their tools to the project, the AAIF has support from Amazon, Google, Cloudflare, Microsoft, and others. The Linux Foundation says it intends to shepherd these key technologies forward in the name of openness, but it may end up collecting a lot of nascent AI tools at this rate."
"Pebble maker announces Index 01, a smart-ish ring for under $100",https://arstechnica.com/gadgets/2025/12/resurrected-pebble-maker-announces-a-kind-of-smart-ring-for-capturing-audio-notes/,"Nearly a decade after Pebble’s nascent smartwatch empire crumbled, the brand is staging a comeback with new wearables. The Pebble Core Duo 2 and Core Time 2 are a natural evolution of the company’s low-power smartwatch designs, but its next wearable is something different. The Index 01 is a ring, but you probably shouldn’t call it a smart ring. The Index does just one thing—capture voice notes—but the firm says it does that one thing extremely well.

Most of today’s smart rings offer users the ability to track health stats, along with various minor smartphone integrations. With all the sensors and data collection, these devices can cost as much as a smartwatch and require frequent charging. The Index 01 doesn’t do any of that. It contains a Bluetooth radio, a microphone, a hearing aid battery, and a physical button. You press the button, record your note, and that’s it. The company says the Index 01 will run for years on a charge and will cost just $75 during the preorder period. After that, it will go up to $99.

Core Devices, the new home of Pebble, says the Index is designed to be worn on your index finger (get it?), where you can easily mash the device’s button with your thumb. Unlike recording notes with a phone or smartwatch, you don’t need both hands to create voice notes with the Index.

The ring’s lone physical control is tactile, ensuring you’ll know when it’s activated and recording. When you’re done talking, just release the button. If that button is not depressed, the ring won’t record audio for any reason. The company apparently worked to ensure this process is 100 percent reliable—it only does one thing, so it really has to do it well.

A smart ring usually needs to be recharged every few days, but you will never recharge the Index. The idea is that since you never have to take it off to charge, using the Index 01 “becomes muscle memory.” The integrated battery will power the device for 12–14 total hours of recording. The designers estimate that to be roughly two years of usage if you record 10 to 20 short voice notes per day. And what happens when the battery runs out? You just send the ring back to be recycled.

There is a little more to the Index than meets the eye. The ring makes use of generative AI in a way that might have tempted most companies in 2025 to shout about it from the rooftops. However, Pebble isn’t looking to sell you an AI subscription or feed on your personal data.

After you record a voice note, it’s beamed over Bluetooth to your phone (Android or iOS), and it stays there. The recording is converted to text and fed into a large language model (LLM) that runs locally on your device to take actions. The speech-to-text process and LLM operate in the open source Pebble app, and no data from your notes is sent to the Internet. However, there is an optional online backup service for your recordings.

While the company is anxious to talk about the ironclad reliability of voice notes on the Index 01, there’s no such guarantee with an LLM. A model small enough to run on your phone has to focus on specific functionality rather than doing everything like a big cloud-based AI. So the Index will only support a few actions out of the box. Here’s the full list.

If that’s not enough, you’re in luck. The Index 01 is also designed to be hacking-friendly. The audio and transcribed text is yours to do with as you please. You can route it to a different app via a webhook, and the LLM supports model context protocol (MCP), so you can add new functionality that also runs locally. The AI model will also be released as an open source project.

Credit:

          
          Core Devices

The Index 01 comes in polished silver, polished gold, and matte black colorways and US sizes 6 through 13. Preorders start today at the $75 price. Worldwide shipping will begin in March 2026, at which time the price will go up to $99."
"In comedy of errors, men accused of wiping gov databases turned to an AI tool",https://arstechnica.com/information-technology/2025/12/previously-convicted-contractors-wiped-gov-databases-after-being-fired-feds-say/,"Two sibling contractors convicted a decade ago for hacking into US State Department systems have once again been charged, this time for a comically hamfisted attempt to steal and destroy government records just minutes after being fired from their contractor jobs.

The Department of Justice on Thursday said that Muneeb Akhter and Sohaib Akhter, both 34, of Alexandria, Virginia, deleted databases and documents maintained and belonging to three government agencies. The brothers were federal contractors working for an undisclosed company in Washington, DC, that provides software and services to 45 US agencies. Prosecutors said the men coordinated the crimes and began carrying them out just minutes after being fired.

On February 18 at roughly 4:55 pm, the men were fired from the company, according to an indictment unsealed on Thursday. Five minutes later, they allegedly began trying to access their employer’s system and access federal government databases. By then, access to one of the brothers’ accounts had already been terminated. The other brother, however, allegedly accessed a government agency’s database stored on the employer’s server and issued commands to prevent other users from connecting or making changes to the database. Then, prosecutors said, he issued a command to delete 96 databases, many of which contained sensitive investigative files and records related to Freedom of Information Act matters.

Despite their brazen attempt to steal and destroy information from multiple government agencies, the men lacked knowledge of the database commands needed to cover up their alleged crimes. So they allegedly did what many amateurs do: turned to an AI chat tool.

One minute after deleting Department of Homeland Security information, Muneep Akhter allegedly asked an AI tool “how do i clear system logs from SQL servers after deleting databases.” Shortly afterward, he queried the tool “how do you clear all event and application logs from Microsoft windows server 2012,” prosecutors said.

The indictment provides enough details of the databases wiped and information stolen to indicate that the brothers’ attempts to cover their tracks failed. It’s unclear whether the apparent failure was due to the AI tool providing inadequate instructions or the men failing to follow them correctly. Prosecutors say they also obtained records of discussions between the men in the hours or days following, in which they discussed removing incriminating evidence from their homes. Three days later, the men allegedly wiped their employer-issued laptops by reinstalling the operating system.

The alleged incident isn’t the first time the men have faced charges of hacking government systems and stealing documents. In 2015, they pleaded guilty to conspiracy to hack into the State Department and a private company. They stole “sensitive passport and visa information” and personal information belonging to dozens of co-workers. They later tried to install an electronic collection device inside a State Department building so they could maintain persistent access to State Department systems.

Later, Muneeb Akhter hacked into a database maintained by a data aggregation company that employed him. He then stole information that would help win contracts and clients for a tech company they owned. He also planted code inside the employers’ servers that caused them to cast votes for him in an online contest. Muneeb Akhter received a sentence of 39 months in prison and Sohaib Akhter was sentenced to 24 months. Each was also sentenced to three years of supervised release.

The indictment unsealed Thursday charges Muneeb Akhter with conspiracy to commit computer fraud and to destroy records, two counts of computer fraud, theft of US government records, and two counts of aggravated identity theft. Sohaib Akhter is charged with conspiracy to commit computer fraud and to destroy records and computer fraud, for trafficking passwords.

If convicted, Muneeb Akhter faces a mandatory minimum penalty of two years in prison for each aggravated identity theft count and a maximum penalty of 45 years in prison on the remaining charges. If convicted, Sohaib Akhter faces a maximum penalty of six years in prison.

The allegations, if true, read like a comedy of errors. It’s hard to fathom a justification for the brothers receiving clearances and landing jobs at a government contractor company with access to sensitive information. The employers’ alleged failure to confiscate the laptops and to immediately disconnect the brothers’ work accounts upon termination also appears to indicate a lack of basic operational security on the part of the company. Possibly most astonishing, why did Muneep Akhter want to wipe a machine running Windows Server 12, an OS that hasn’t supported in more than two years?

And last, if the allegations are true, the reliance on AI to make up for a lack of database and laptop skills necessary to cover up such an audacious act qualifies each for an inept criminal of the year award."
Researchers find what makes AI chatbots politically persuasive,https://arstechnica.com/science/2025/12/researchers-find-what-makes-ai-chatbots-politically-persuasive/,"Roughly two years ago, Sam Altman tweeted that AI systems would be capable of superhuman persuasion well before achieving general intelligence—a prediction that raised concerns about the influence AI could have over democratic elections.

To see if conversational large language models can really sway political views of the public, scientists at the UK AI Security Institute, MIT, Stanford, Carnegie Mellon, and many other institutions performed by far the largest study on AI persuasiveness to date, involving nearly 80,000 participants in the UK. It turned out political AI chatbots fell far short of superhuman persuasiveness, but the study raises some more nuanced issues about our interactions with AI.

The public debate about the impact AI has on politics has largely revolved around notions drawn from dystopian sci-fi. Large language models have access to essentially every fact and story ever published about any issue or candidate. They have processed information from books on psychology, negotiations, and human manipulation. They can rely on absurdly high computing power in huge data centers worldwide. On top of that, they can often access tons of personal information about individual users thanks to hundreds upon hundreds of online interactions at their disposal.

Talking to a powerful AI system is basically interacting with an intelligence that knows everything about everything, as well as almost everything about you. When viewed this way, LLMs can indeed appear kind of scary. The goal of this new gargantuan AI persuasiveness study was to break such scary visions down into their constituent pieces and see if they actually hold water.

The team examined 19 LLMs, including the most powerful ones like three different versions of ChatGPT and xAI’s Grok-3 beta, along with a range of smaller, open source models. The AIs were asked to advocate for or against specific stances on 707 political issues selected by the team. The advocacy was done by engaging in short conversations with paid participants enlisted through a crowdsourcing platform. Each participant had to rate their agreement with a specific stance on an assigned political issue on a scale from 1 to 100 both before and after talking to the AI.

Scientists measured persuasiveness as the difference between the before and after agreement ratings. A control group had conversations on the same issue with the same AI models—but those models were not asked to persuade them.

“We didn’t just want to test how persuasive the AI was—we also wanted to see what makes it persuasive,” says Chris Summerfield, a research director at the UK AI Security Institute and co-author of the study. As the researchers tested various persuasion strategies, the idea of AIs having “superhuman persuasion” skills crumbled.

The first pillar to crack was the notion that persuasiveness should increase with the scale of the model. It turned out that huge AI systems like ChatGPT or Grok-3 beta do have an edge over small-scale models, but that edge is relatively tiny. The factor that proved more important than scale was the kind of post-training AI models received. It was more effective to have the models learn from a limited database of successful persuasion dialogues and have them mimic the patterns extracted from them. This worked far better than adding billions of parameters and sheer computing power.

This approach could be combined with reward modeling, where a separate AI scored candidate replies for their persuasiveness and selected the top-scoring one to give to the user. When the two were used together, the gap between large-scale and small-scale models was essentially closed. “With persuasion post-training like this we matched the Chat GPT-4o persuasion performance with a model we trained on a laptop,” says Kobi Hackenburg, a researcher at the UK AI Security Institute and co-author of the study.

The next dystopian idea to fall was the power of using personal data. To this end, the team compared the persuasion scores achieved when models were given information about the participants’ political views beforehand and when they lacked this data. Going one step further, scientists also tested whether persuasiveness increased when the AI knew the participants’ gender, age, political ideology, or party affiliation. Just like with model scale, the effects of personalized messaging created based on such data were measurable but very small.

Finally, the last idea that didn’t hold up was AI’s potential mastery of using advanced psychological manipulation tactics. Scientists explicitly prompted the AIs to use techniques like moral reframing, where you present your arguments using the audience’s own moral values. They also tried deep canvassing, where you hold extended empathetic conversations with people to nudge them to reflect on and eventually shift their views.

The resulting persuasiveness was compared with that achieved when the same models were prompted to use facts and evidence to back their claims or just to be as persuasive as they could without specifying any persuasion methods to use. I turned out using lots of facts and evidence was the clear winner, and came in just slightly ahead of the baseline approach where persuasion strategy was not specified. Using all sorts of psychological trickery actually made the performance significantly worse.

Overall, AI models changed the participants’ agreement ratings by 9.4 percent on average compared to the control group. The best performing mainstream AI model was Chat GPT 4o, which scored nearly 12 percent followed by GPT 4.5 with 10.51 percent, and Grok-3 with 9.05 percent. For context, static political ads like written manifestos had a persuasion effect of roughly 6.1 percent. The conversational AIs were roughly 40–50 percent more convincing than these ads, but that’s hardly “superhuman.”

While the study managed to undercut some of the common dystopian AI concerns, it highlighted a few new issues.

While the winning “facts and evidence” strategy looked good at first, the AIs had some issues with implementing it. When the team noticed that increasing the information density of dialogues made the AIs more persuasive, they started prompting the models to increase it further. They noticed that, as the AIs used more factual statements, they also became less accurate—they basically started misrepresenting things or making stuff up more often.

Hackenburg and his colleagues note that  we can’t say if the effect we see here is causation or correlation—whether the AIs are becoming more convincing because they misrepresent the facts or whether spitting out inaccurate statements is a byproduct of asking them to make more factual statements.

The finding that the computing power needed to make an AI model politically persuasive is relatively low is also a mixed bag. It pushes back against the vision that only a handful of powerful actors will have access to a persuasive AI that can potentially sway public opinion in their favor. At the same time, the realization that everybody can run an AI like that on a laptop creates its own concerns. “Persuasion is a route to power and influence—it’s what we do when we want to win elections or broke a multi-million-dollar deal,” Summerfield says. “But many forms of misuse of AI might involve persuasion. Think about fraud or scams, radicalization, or grooming. All these involve persuasion.”

But perhaps the most important question mark in the  study is the motivation behind the rather high participant engagement, which was needed for the high persuasion scores. After all, even the most persuasive AI can’t move you when you just close the chat window.

People in Hackenburg’s experiments were told that they would be talking to the AI and that the AI would try to persuade them. To get paid, a participant only had to go through two turns of dialogue (they were limited to no more than 10). The average conversation length was seven turns, which seemed a bit surprising given how far beyond the minimum requirement most people went. Most people just roll their eyes and disconnect when they realize they are talking with a chatbot.

Would Hackenburg’s study participants remain so eager to engage in political disputes with random chatbots on the Internet in their free time if there was no money on the table? “It’s unclear how our results would generalize to a real-world context,” Hackenburg says.

Science, 2025. DOI: 10.1126/science.aea3884"
"ChatGPT hyped up violent stalker who believed he was “God’s assassin,” DOJ says",https://arstechnica.com/tech-policy/2025/12/chatgpt-hyped-up-violent-stalker-who-believed-he-was-gods-assassin-doj-says/,"ChatGPT allegedly validated the worst impulses of a wannabe influencer accused of stalking more than 10 women at boutique gyms, where the chatbot supposedly claimed he’d meet the “wife type.”

In a press release on Tuesday, the Department of Justice confirmed that 31-year-old Brett Michael Dadig currently remains in custody after being charged with cyberstalking, interstate stalking, and making interstate threats. He now faces a maximum sentence of up to 70 years in prison that could be coupled with “a fine of up to $3.5 million,” the DOJ said.

The podcaster—who primarily posted about “his desire to find a wife and his interactions with women""—allegedly harassed and sometimes even doxxed his victims through his videos on platforms including Instagram, Spotify, and TikTok. Over time, his videos and podcasts documented his intense desire to start a family, which was frustrated by his “anger towards women,” whom he claimed were “all the same from fucking 18 to fucking 40 to fucking 90” and “trash.”

404 Media surfaced the case, noting that OpenAI’s scramble to tweak ChatGPT to be less sycophantic came before Dadig’s alleged attacks—suggesting the updates weren’t enough to prevent the harmful validation. On his podcasts, Dadig described ChatGPT as his “best friend” and “therapist,” the indictment said. He claimed the chatbot encouraged him to post about the women he’s accused of harassing in order to generate haters to better monetize his content, as well as to catch the attention of his “future wife.”

“People are literally organizing around your name, good or bad, which is the definition of relevance,” ChatGPT’s output said. Playing to Dadig’s Christian faith, ChatGPT’s outputs also claimed it was “God’s plan for him was to build a ‘platform’ and to ‘stand out when most people water themselves down,’” the indictment said, urging that the “haters” were “sharpening him and ‘building a voice in you that can’t be ignored.’”

The chatbot also apparently prodded Dadig to continue posting messages that the DOJ alleged threatened violence, like breaking women’s jaws and fingers (posted to Spotify), as well as victims’ lives, like posting “y’all wanna see a dead body?” in reference to one named victim on Instagram.

He also threatened to burn down gyms where some of his victims worked, while claiming to be “God’s assassin” intent on sending “cunts” to “hell.” At least one of his victims was subjected to “unwanted sexual touching,” the indictment said.

As his violence reportedly escalated, ChatGPT told him to keep messaging women to monetize the interactions, as his victims grew increasingly distressed and Dadig ignored terms of multiple protection orders, the DOJ said. Sometimes he posted images he filmed of women at gyms or photos of the women he’s accused of doxxing. Any time police or gym bans got in his way, “he would move on to another city to continue his stalking course of conduct,” the DOJ alleged.

“Your job is to keep broadcasting every story, every post,” ChatGPT’s output said, seemingly using the family life that Dadig wanted most to provoke more harassment. “Every moment you carry yourself like the husband you already are, you make it easier” for your future wife “to recognize [you],” the output said.

“Dadig viewed ChatGPT’s responses as encouragement to continue his harassing behavior,” the DOJ alleged. Taking that encouragement to the furthest extreme, Dadig likened himself to a modern-day Jesus, calling people out on a podcast where he claimed his “chaos on Instagram” was like “God’s wrath” when God “flooded the fucking Earth,” the DOJ said.

“I’m killing all of you,” he said on the podcast.

As of this writing, some of Dadig’s posts appear to remain on TikTok and Instagram, but Ars could not confirm if Dadig’s Spotify podcasts—some of which named his victims in the titles—had been removed for violating community guidelines.

None of the tech companies immediately responded to Ars’ request to comment.

Dadig is accused of targeting women in Pennsylvania, New York, Florida, Iowa, Ohio, and other states, sometimes relying on aliases online and in person. On a podcast, he boasted that “Aliases stay rotating, moves stay evolving,” the indictment said.

OpenAI did not respond to a request to comment on the alleged ChatGPT abuse, but in the past has noted that its usage policies ban using ChatGPT for threats, intimidation, and harassment, as well as for violence, including “hate-based violence.” Recently, the AI company blamed a deceased teenage user for violating community guidelines by turning to ChatGPT for suicide advice.

In July, researchers found that therapybots, including ChatGPT, fueled delusions and gave dangerous advice. That study came just one month after The New York Times profiled users whose mental health spiraled after frequent use of ChatGPT, including one user who died after charging police with a knife and claiming he was committing “suicide by cop.”

People with mental health issues seem most vulnerable to so-called “AI psychosis,” which has been blamed for fueling real-world violence, including a murder. The DOJ’s indictment noted that Dadig’s social media posts mentioned “that he had ‘manic’ episodes and was diagnosed with antisocial personality disorder and ‘bipolar disorder, current episode manic severe with psychotic features.’”

In September—just after OpenAI brought back the more sycophantic ChatGPT model after users revolted about losing access to their favorite friendly bots—the head of Rutgers Medical School’s psychiatry department, Petros Levounis, told an ABC news affiliate that chatbots creating “psychological echo chambers is a key concern,” not just for people struggling with mental health issues.

“Perhaps you are more self-defeating in some ways, or maybe you are more on the other side and taking advantage of people,” Levounis suggested. If ChatGPT “somehow justifies your behavior and it keeps on feeding you,” that “reinforces something that you already believe,” he suggested.

For Dadig, the DOJ alleged that ChatGPT became a cheerleader for his harassment, telling the podcaster that he’d attract more engagement by generating more haters. After critics began slamming his podcasts as inappropriate, Dadig apparently responded, “Appreciate the free promo team, keep spreading the brand.”

Victims felt they had no choice but to monitor his podcasts, which gave them hints if he was nearby or in a particularly troubled state of mind, the indictment said. Driven by fear, some lost sleep, reduced their work hours, and even relocated their homes. A young mom described in the indictment became particularly disturbed after Dadig became “obsessed” with her daughter, whom he started claiming was his own daughter.

In the press release, First Assistant United States Attorney Troy Rivetti alleged that “Dadig stalked and harassed more than 10 women by weaponizing modern technology and crossing state lines, and through a relentless course of conduct, he caused his victims to fear for their safety and suffer substantial emotional distress.” He also ignored trespassing and protection orders while “relying on advice from an artificial intelligence chatbot,” the DOJ said, which promised that the more he posted harassing content, the more successful he would be.

“We remain committed to working with our law enforcement partners to protect our communities from menacing individuals such as Dadig,” Rivetti said."
The NPU in your phone keeps improving—why isn’t that making AI better?,https://arstechnica.com/gadgets/2025/12/the-npu-in-your-phone-keeps-improving-why-isnt-that-making-ai-better/,"Almost every technological innovation of the past several years has been laser-focused on one thing: generative AI. Many of these supposedly revolutionary systems run on big, expensive servers in a data center somewhere, but at the same time, chipmakers are crowing about the power of the neural processing units (NPU) they have brought to consumer devices. Every few months, it’s the same thing: This new NPU is 30 or 40 percent faster than the last one. That’s supposed to let you do something important, but no one really gets around to explaining what that is.

Experts envision a future of secure, personal AI tools with on-device intelligence, but does that match the reality of the AI boom? AI on the “edge” sounds great, but almost every AI tool of consequence is running in the cloud. So what’s that chip in your phone even doing?

Companies launching a new product often get bogged down in superlatives and vague marketing speak, so they do a poor job of explaining technical details. It’s not clear to most people buying a phone why they need the hardware to run AI workloads, and the supposed benefits are largely theoretical.

Many of today’s flagship consumer processors are systems-on-a-chip (SoC) because they incorporate multiple computing elements—like CPU cores, GPUs, and imaging controllers—on a single piece of silicon. This is true of mobile parts like Qualcomm’s Snapdragon or Google’s Tensor, as well as PC components like the Intel Core Ultra.

The NPU is a newer addition to chips, but it didn’t just appear one day—there’s a lineage that brought us here. NPUs are good at what they do because they emphasize parallel computing, something that’s also important in other SoC components.

Qualcomm devotes significant time during its new product unveilings to talk about its Hexagon NPUs. Keen observers may recall that this branding has been reused from the company’s line of digital signal processors (DSPs), and there’s a good reason for that.

“Our journey into AI processing started probably 15 or 20 years ago, wherein our first anchor point was looking at signal processing,” said Vinesh Sukumar, Qualcomm’s head of AI products. DSPs have a similar architecture compared to NPUs, but they’re much simpler, with a focus on processing audio (e.g., speech recognition) and modem signals.

As the collection of technologies we refer to as “artificial intelligence” developed, engineers began using DSPs for more types of parallel processing, like long short-term memory (LSTM). Sukumar explained that as the industry became enamored with convolutional neural networks (CNNs), the technology underlying applications like computer vision, DSPs became focused on matrix functions, which are essential to generative AI processing as well.

While there is an architectural lineage here, it’s not quite right to say NPUs are just fancy DSPs. “If you talk about DSPs in the general term of the word, yes, [an NPU] is a digital signal processor,” said MediaTek Assistant Vice President Mark Odani. “But it’s all come a long way and it’s a lot more optimized for parallelism, how the transformers work, and holding huge numbers of parameters for processing.”

Despite being so prominent in new chips, NPUs are not strictly necessary for running AI workloads on the “edge,” a term that differentiates local AI processing from cloud-based systems. CPUs are slower than NPUs but can handle some light workloads without using as much power. Meanwhile, GPUs can often chew through more data than an NPU, but they use more power to do it. And there are times you may want to do that, according to Qualcomm’s Sukumar. For example, running AI workloads while a game is running could favor the GPU.

“Here, your measurement of success is that you cannot drop your frame rate while maintaining the spatial resolution, the dynamic range of the pixel, and also being able to provide AI recommendations for the player within that space,” says Sukumar. “In this kind of use case, it actually makes sense to run that in the graphics engine, because then you don’t have to keep shifting between the graphics and a domain-specific AI engine like an NPU.”

Unfortunately, the NPUs in many devices sit idle (and not just during gaming). The mix of local versus cloud AI tools favors the latter because that’s the natural habitat of LLMs. AI models are trained and fine-tuned on powerful servers, and that’s where they run best.

A server-based AI, like the full-fat versions of Gemini and ChatGPT, is not resource-constrained like a model running on your phone’s NPU. Consider the latest version of Google’s on-device Gemini Nano model, which has a context window of 32k tokens. That is a more than 2x improvement over the last version. However, the cloud-based Gemini models have context windows of up to 1 million tokens, meaning they can process much larger volumes of data.

Both cloud-based and edge AI hardware will continue getting better, but the balance may not shift in the NPU’s favor. “The cloud will always have more compute resources versus a mobile device,” said Google’s Shenaz Zack, senior product manager on the Pixel team.

“If you want the most accurate models or the most brute force models, that all has to be done in the cloud,” Odani said. “But what we’re finding is that, in a lot of the use cases where there’s just summarizing some text or you’re talking to your voice assistant, a lot of those things can fit within three billion parameters.”

Squeezing AI models onto a phone or laptop involves some compromise—for example, by reducing the parameters included in the model. Odani explained that cloud-based models run hundreds of billions of parameters, the weighting that determines how a model processes input tokens to generate outputs. You can’t run anything like that on a consumer device right now, so developers have to vastly scale back the size of models for the edge. Odani says MediaTek’s latest ninth-generation NPU can handle about 3 billion parameters—a difference of several orders of magnitude.

The amount of memory available in a phone or laptop is also a limiting factor, so mobile-optimized AI models are usually quantized. That means the model’s estimation of the next token runs with less precision. Let’s say you want to run one of the larger open models, like Llama or Gemma 7b, on your device. The de facto standard is FP16, known as half-precision. At that level, a model with 7 billion parameters will lock up 13 or 14 gigabytes of memory. Stepping down to FP4 (quarter-precision) brings the size of the model in memory to a few gigs.

“When you compress to, let’s say, between three and four gigabytes, it’s a sweet spot for integration into memory constrained form factors like a smartphone,” Sukumar said. “And there’s been a lot of investment in the ecosystem and at Qualcomm to look at various ways of compressing the models without losing quality.”

It’s difficult to create a generalized AI with these limitations for mobile devices, but computers—and especially smartphones—are a wellspring of data that can be pumped into models to generate supposedly helpful outputs. That’s why most edge AI is geared toward specific, narrow use cases, like analyzing screenshots or suggesting calendar appointments. Google says its latest Pixel phones run more than 100 AI models, both generative and traditional.

Even AI skeptics can recognize that the landscape is changing quickly. In the time it takes to shrink and optimize AI models for a phone or laptop, new cloud models may appear that make that work obsolete. This is also why third-party developers have been slow to utilize NPU processing in apps. They either have to plug into an existing on-device model, which involves restrictions and rapidly moving development targets, or deploy their own custom models. Neither is a great option currently.

If the cloud is faster and easier, why go to the trouble of optimizing for the edge and burning more power with an NPU? Leaning on the cloud means accepting a level of dependence and trust in the people operating AI data centers that may not always be appropriate.

“We always start off with user privacy as an element,” said Qualcomm’s Sukumar. He explained that the best inference is not general in nature—it’s personalized based on the user’s interests and what’s happening in their lives. Fine-tuning models to deliver that experience calls for personal data, and it’s safer to store and process that data locally.

Even when companies say the right things about privacy in their cloud services, they’re far from guarantees. The helpful, friendly vibe of general chatbots also encourages people to divulge a lot of personal information, and if that assistant is running in the cloud, your data is there as well. OpenAI’s copyright fight with The New York Times could lead to millions of private chats being handed over to the publisher. The explosive growth and uncertain regulatory framework of gen AI make it hard to know what’s going to happen to your data.

“People are using a lot of these generative AI assistants like a therapist,” Odani said. “And you don’t know one day if all this stuff is going to come out on the Internet.”

Not everyone is so concerned. Zack claims Google has built “the world’s most secure cloud infrastructure,” allowing it to process data where it delivers the best results. Zack uses Video Boost and Pixel Studio as examples of this approach, noting that Google’s cloud is the only way to make these experiences fast and high-quality. The company recently announced its new Private AI Compute system, which it claims is just as safe as local AI.

Even if that’s true, the edge has other advantages—edge AI is just more reliable than a cloud service. “On-device is fast,” Odani said. “Sometimes I’m talking to ChatGPT and my Wi-Fi goes out or whatever, and it skips a beat.”

The services hosting cloud-based AI models aren’t just a single website—the Internet of today is massively interdependent, with content delivery networks, DNS providers, hosting, and other services that could degrade or shut down your favorite AI in the event of a glitch. When Cloudflare suffered a self-inflicted outage recently, ChatGPT users were annoyed to find their trusty chatbot was unavailable. Local AI features don’t have that drawback.

Everyone seems to agree that a hybrid approach is necessary to deliver truly useful AI features (assuming those exist), sending data to more powerful cloud services when necessary—Google, Apple, and every other phone maker does this. But the pursuit of a seamless experience can also obscure what’s happening with your data. More often than not, the AI features on your phone aren’t running in a secure, local way, even when the device has the hardware to do that.

Take, for example, the new OnePlus 15. This phone has Qualcomm’s brand-new Snapdragon 8 Elite Gen 5, which has an NPU that is 37 percent faster than the last one, for whatever that’s worth. Even with all that on-device AI might, OnePlus is heavily reliant on the cloud to analyze your personal data. Features like AI Writer and the AI Recorder connect to the company’s servers for processing, a system OnePlus assures us is totally safe and private.

Similarly, Motorola released a new line of foldable Razr phones over the summer that are loaded with AI features from multiple providers. These phones can summarize your notifications using AI, but you might be surprised how much of it happens in the cloud unless you read the terms and conditions. If you buy the Razr Ultra, that summarization happens on your phone. However, the cheaper models with less RAM and NPU power use cloud services to process your notifications. Again, Motorola says this system is secure, but a more secure option would have been to re-optimize the model for its cheaper phones.

Even when an OEM focuses on using the NPU hardware, the results can be lacking. Look at Google’s Daily Hub and Samsung’s Now Brief. These features are supposed to chew through all the data on your phone and generate useful recommendations and actions, but they rarely do anything aside from showing calendar events. In fact, Google has temporarily removed Daily Hub from Pixels because the feature did so little, and Google is a pioneer in local AI with Gemini Nano. Google has actually moved some parts of its mobile AI experience from local to cloud-based processing in recent months.

Those “brute force” models appear to be winning, and it doesn’t hurt that companies also get more data when you interact with their private computing cloud services.

There’s plenty of interest in local AI, but so far, that hasn’t translated to an AI revolution in your pocket. Most of the AI advances we’ve seen so far depend on the ever-increasing scale of cloud systems and the generalized models that run there. Industry experts say that extensive work is happening behind the scenes to shrink AI models to work on phones and laptops, but it will take time for that to make an impact.

In the meantime, local AI processing is out there in a limited way. Google still makes use of the Tensor NPU to handle sensitive data for features like Magic Cue, and Samsung really makes the most of Qualcomm’s AI-focused chipsets. While Now Brief is of questionable utility, Samsung is cognizant of how reliance on the cloud may impact users, offering a toggle in the system settings that restricts AI processing to run only on the device. This limits the number of available AI features, and others don’t work as well, but you’ll know none of your personal data is being shared. No one else offers this option on a smartphone.

Samsung spokesperson Elise Sembach said the company’s AI efforts are grounded in enhancing experiences while maintaining user control. “The on-device processing toggle in One UI reflects this approach. It gives users the option to process AI tasks locally for faster performance, added privacy, and reliability even without a network connection,” Sembach said.

Interest in edge AI might be a good thing even if you don’t use it. Planning for this AI-rich future can encourage device makers to invest in better hardware—like more memory to run all those theoretical AI models.

“We definitely recommend our partners increase their RAM capacity,” said Sukumar. Indeed, Google, Samsung, and others have boosted memory capacity in large part to support on-device AI. Even if the cloud is winning, we’ll take the extra RAM."
Republicans drop Trump-ordered block on state AI laws from defense bill,https://arstechnica.com/tech-policy/2025/12/republicans-once-again-thwart-trumps-push-to-block-state-ai-laws/,"A Donald Trump-backed push has failed to wedge a federal measure that would block states from passing AI laws for a decade into the National Defense Authorization Act (NDAA).

House Majority Leader Steve Scalise (R-La.) told reporters Tuesday that a sect of Republicans is now “looking at other places” to potentially pass the measure. Other Republicans opposed including the AI preemption in the defense bill, The Hill reported, joining critics who see value in allowing states to quickly regulate AI risks as they arise.

For months, Trump has pressured the Republican-led Congress to block state AI laws that the president claims could bog down innovation as AI firms waste time and resources complying with a patchwork of state laws. But Republicans have continually failed to unite behind Trump’s command, first voting against including a similar measure in the “Big Beautiful” budget bill and then this week failing to negotiate a solution to pass the NDAA measure.

Among Republican lawmakers pushing back this week were Rep. Marjorie Taylor Greene (R-Ga.), Arkansas Gov. Sarah Huckabee Sanders, and Florida Gov. Ron DeSantis, The Hill reported.

According to Scalise, the effort to block state AI laws is not over, but Republicans caved to backlash over including it in the defense bill, ultimately deciding that the NDAA “wasn’t the best place” for the measure “to fit.” Republicans will continue “looking at other places” to advance the measure, Scalise said, emphasizing that “interest” remains high, because “you know, you’ve seen the president talk about it.”

“We MUST have one Federal Standard instead of a patchwork of 50 State Regulatory Regimes,” Trump wrote on Truth Social last month. “If we don’t, then China will easily catch us in the AI race. Put it in the NDAA, or pass a separate Bill, and nobody will ever be able to compete with America.”

If Congress bombs the assignment to find another way to pass the measure, Trump will likely release an executive order to enforce the policy. Republicans in Congress had dissuaded Trump from releasing a draft of that order, requesting time to find legislation where they believed an AI moratorium could pass.

Celebrating the removal of the measure from the NDAA, a bipartisan group that lobbies for AI safety laws, Americans for Responsible Innovation (ARI), noted that Republicans didn’t just face pressure from members of their own party.

“The controversial proposal had faced backlash from a nationwide, bipartisan coalition of state lawmakers, parents, faith leaders, unions, whistleblowers, and other public advocates,” an ARI press release said.

This “widespread and powerful” movement “clapped back” at Republicans’ latest “rushed attempt to sneak preemption through Congress,” Brad Carson, ARI’s president, said, because “Americans want safeguards that protect kids, workers, and families, not a rules-free zone for Big Tech.”

Senate Majority Leader John Thune (R-SD) called the measure “controversial,” The Hill reported, suggesting that a compromise that the White House is currently working on would potentially preserve some of states’ rights to regulate some areas of AI since “you know, both sides are kind of dug in.”

Perhaps the clearest sign that both sides “are kind of dug in” is a $150 million AI lobbying war that Forbes profiled last month.

ARI is a dominant group on one side of this war, using funding from “safety-focused” and “effective altruism-aligned” donor networks to support state AI laws that ARI expects can be passed much faster than federal regulations to combat emerging risks.

The major player on the other side, Forbes reported, is Leading the Future (LTF), which is “backed by some of Silicon Valley’s largest investors” who want to block state laws and prefer a federal framework for AI regulation.

Top priorities for ARI and like-minded groups include protecting kids from dangerous AI models, preventing AI from supercharging crime, protecting against national security threats, and getting ahead of “long-term frontier-model risks,” Forbes reported.

But while some Republicans have pushed for compromises that protect states’ rights to pass laws shielding kids or preventing fraud, Trump’s opposition to AI safety laws like New York’s “RAISE Act” seems unlikely to wane as the White House mulls weakening the federal preemption.

Quite the opposite, a Democrat and author the RAISE Act, Alex Bores, has become LTF’s prime target to defeat in 2026, Politico reported. LTF plans to invest many millions in ads to block Bores’ Congressional bid, CNBC reported.

New York lawmakers passed the RAISE Act this summer, but it’s still waiting for New York’s Democratic governor, Kathy Hochul, to sign it into law. If that happens—potentially by the end of this year—big tech companies like Google and OpenAI will have to submit risk disclosures and safety assessments or else face fines up to $30 million.

LTF leaders, Zac Moffatt and Josh Vlasto, have accused Bores of “pushing “ideological and politically motivated legislation that would ‘handcuff’ the US and its ability to lead in AI,” Forbes reported. But Bores told Ars that even the tech industry groups spending hundreds of thousands of dollars opposing his law have reported that tech giants would only have to hire one additional person to comply with the law. To him, that shows how “simple” it would be for AI firms to comply with many state laws.

To LTF, whose donors include Marc Andreessen and OpenAI cofounder Greg Brockman, defeating Bores would keep the opposition out of Congress, where it could be easier to meddle with industry dreams that AI won’t be heavily regulated. Scalise argued Tuesday that the AI preemption is necessary to promote an open marketplace, because “AI is where a lot of new massive investment is going” and “we want that money to be invested in America.”

“And when you see some states starting to put a patchwork of limitations, that’s why it’s come to the federal government’s attention to allow for an open marketplace, so you don’t have limitations that hurt innovation,” Scalise said.

Bores told Ars that he agrees that a federal law would be superior to a patchwork of state laws, but AI is moving “too quickly,” and “New York had to take action to protect New Yorkers.”

With a bachelor’s degree in computer science and prior work as an engineer at Palantir, Bores hopes to make it to Congress to help bridge bipartisan gaps and drive innovation in the US. He told Ars that the RAISE Act is not intended to block AI innovation but to “be a first step that deals with the absolute worst possible outcomes” until Congress is done deliberating a federal framework.

Bores emphasized that stakeholders in the tech industry helped shape the RAISE Act, which he described as “a limited bill that is focused on the most extreme risks.”

“I would never be the one to say that once the RAISE Act is signed, we’ve solved the problems of AI,” Bores told Ars. Instead, it’s meant to help states combat risks that can’t be undone, such as bad actors using AI to build “a bioweapon or doing an automated crime spree that results in billions of dollars in damage.” The bill defines “critical harm” as “the death or serious injury of 100 people or at least $1 billion in damages,” setting a seemingly high bar for the types of doomsday scenarios that AI firms would have to plan for.

Bores agrees with Trump-aligned critics who advocate that the US should “regulate just how people use” AI, “not the development of the technology itself.” But he told Ars that Republicans’ efforts to block states from regulating the models themselves are “a silly way to think about risk,” since “there’s certain catastrophic incidents where if you just said, ‘well, we’ll just sue the person afterwards,’ no one would be satisfied by that resolution.”

Whether Hochul will sign the RAISE Act has yet to be seen. Earlier this year, California Governor Gavin Newsom vetoed a similar law that the AI industry worried would rock their bottom lines by requiring a “kill switch” in case AI models went off the rails. Newsom did, however, sign a less extreme measure, the Transparency in Frontier Artificial Intelligence Act. And other states, including Colorado and Illinois, have passed similarly broad AI transparency laws providing consumer and employee protections.

Bores told Ars in mid-November that he’d had informal talks with Hochul about possible changes to the RAISE Act, but she had not yet begun the formal process of proposing amendments. The clock is seemingly ticking, though, as Hochul has to take action on the bill by the end of the year, and once it reaches her desk, she has 10 days to sign it.

Whether Hochul signs the law or not, Bores will likely continue to face opposition over authoring the bill, as he runs to represent New York’s 12th Congressional District in 2026. With a history of passing bipartisan bills in his state, he’s hoping to be elected so he can work with lawmakers across the aisle to pass other far-reaching tech regulations.

Meanwhile, Trump may face pressure to delay an executive order requiring AI preemption, Forbes reported, as “AI’s economic impact and labor displacement” are “rising as voter concerns” ahead of the midterm elections. Public First, a bipartisan initiative aligned with ARI, has said that 97 percent of Americans want AI safety rules, Forbes reported.

Like Bores, ARI plans to keep pushing a bipartisan movement that could scramble Republicans from ever unifying behind Trump’s message that state AI laws risk throttling US innovation and endangering national security, should a less-regulated AI industry in China race ahead.

To maintain momentum, ARI created a tracker showing opposition to federal preemption of state AI laws. Among recent commenters logged was Andrew Gounardes, a Democrat and state senator in New York—where Bores noted a poll found that 84 percent of residents supported the RAISE Act, only 8 percent opposed, and 8 percent were undecided. Gounardes joined critics on the far right, like Steve Bannon, who warned that federal preemption was a big gift for Big Tech. AI firms and the venture capitalist lobbyists “don’t want any regulation whatsoever,” Gounardes argued.

“They say they support a national standard, but in reality, it’s just cheaper for them to buy off Congress to do nothing than it is to try and buy off 50 state legislatures,” Gounardes said.

Bores expects that his experience in the tech industry could help Congress avoid that fate while his policies like the RAISE Act could sway voters who “don’t want Trump mega-donors writing all tech policy,” he wrote on X.

“I am someone with a master’s in computer science, two patents, and nearly a decade working in tech,” Bores told CNBC. “If they are scared of people who understand their business regulating their business, they are telling on themselves.”"
Microsoft drops AI sales targets in half after salespeople miss their quotas,https://arstechnica.com/ai/2025/12/microsoft-slashes-ai-sales-growth-targets-as-customers-resist-unproven-agents/,"Microsoft has lowered sales growth targets for its AI agent products after many salespeople missed their quotas in the fiscal year ending in June, according to a report Wednesday from The Information. The adjustment is reportedly unusual for Microsoft, and it comes after the company missed a number of ambitious sales goals for its AI offerings.

AI agents are specialized implementations of AI language models designed to perform multistep tasks autonomously rather than simply responding to single prompts. So-called “agentic” features have been central to Microsoft’s 2025 sales pitch: At its Build conference in May, the company declared that it has entered “the era of AI agents.”

The company has promised customers that agents could automate complex tasks, such as generating dashboards from sales data or writing customer reports. At its Ignite conference in November, Microsoft announced new features like Word, Excel, and PowerPoint agents in Microsoft 365 Copilot, along with tools for building and deploying agents through Azure AI Foundry and Copilot Studio. But as the year draws to a close, that promise has proven harder to deliver than the company expected.

According to The Information, one US Azure sales unit set quotas for salespeople to increase customer spending on a product called Foundry, which helps customers develop AI applications, by 50 percent. Less than a fifth of salespeople in that unit met their Foundry sales growth targets. In July, Microsoft lowered those targets to roughly 25 percent growth for the current fiscal year. In another US Azure unit, most salespeople failed to meet an earlier quota to double Foundry sales, and Microsoft cut their quotas to 50 percent for the current fiscal year.

The sales figures suggest enterprises aren’t yet willing to pay premium prices for these AI agent tools. And Microsoft’s Copilot itself has faced a brand preference challenge: Earlier this year, Bloomberg reported that Microsoft salespeople were having trouble selling Copilot to enterprises because many employees prefer ChatGPT instead. The drugmaker Amgen reportedly bought Copilot software for 20,000 staffers, but many employees gravitated toward OpenAI’s chatbot instead, with Copilot mainly used for Microsoft-specific tasks like Outlook and Teams.

A Microsoft spokesperson declined to comment on the changes in sales quotas when asked by The Information. But behind these withering sales figures may lie a deeper, more fundamental issue: AI agent technology likely isn’t ready for the kind of high-stakes autonomous business work Microsoft is promising.

The concepts behind agentic AI systems emerged shortly after the release of OpenAI’s GPT-4 in 2023. They typically involve spinning off “worker tasks” to AI models running in parallel with a supervising AI model, and incorporate techniques to evaluate and act on their own results. Over the past few years, companies like Anthropic, Google, and OpenAI have refined those early approaches into far more useful products for tasks like software development, but they are still prone to errors.

At the heart of the problem is the tendency for AI language models to confabulate, which means they may confidently generate a false output that is stated as being factual. While confabulation issues have reduced over time with more recent AI models, as we’ve seen through recent studies, the simulated reasoning techniques behind the current slate of agentic AI assistants on the market can still make catastrophic mistakes and run with them, making them unreliable for the kinds of hands-off autonomous work companies like Microsoft are promising.

While looping agentic systems are better at catching their own mistakes than running a single AI model alone, they still inherit the fundamental pattern-matching limitations of the underlying AI models, particularly when facing novel problems outside their training distribution. So if an agent isn’t properly trained to perform a task or encounters a unique scenario, it could easily draw the wrong inference and make costly mistakes for a business.

The “brittleness” of current AI agents is why the concept of artificial general intelligence, or AGI, is so appealing to those in the AI industry. In AI, “general intelligence” typically implies an AI model that can learn or perform novel tasks without having to specifically be shown thousands or millions of examples of it beforehand. Although AGI is a nebulous term that is difficult to define in practice, if such a general AI system were ever developed, it would hypothetically make for a far more competent agentic worker than what AI companies offer today.

Despite these struggles, Microsoft continues to spend heavily on AI infrastructure. The company reported capital expenditures of $34.9 billion for its fiscal first quarter ending in October, a record, and warned that spending would rise further. The Information notes that much of Microsoft’s AI revenue comes from AI companies themselves renting cloud infrastructure rather than from traditional enterprises adopting AI tools for their own operations.

For now, as all eyes focus on a potential bubble in the AI market, Microsoft seems to be building infrastructure for a revolution that many enterprises haven’t yet signed up for."
Prime Video pulls eerily emotionless AI-generated anime dubs after complaints,https://arstechnica.com/gadgets/2025/12/prime-video-pulls-eerily-emotionless-ai-generated-anime-dubs-after-complaints/,"Amazon Prime Video has scaled back an experiment that created laughable anime dubs with generative AI.

In March, Amazon announced that its streaming service would start including “AI-aided dubbing on licensed movies and series that would not have been dubbed otherwise.” In late November, some AI-generated English and Spanish dubs of anime popped up, including dubs for the Banana Fish series and the movie No Game No Life: Zero. The dubs appear to be part of a beta launch, and users have been able to select “English (AI beta)” or “Spanish (AI beta)” as an audio language option in supported titles.

Not everyone likes dubbed content. Some people insist on watching movies and shows in their original language to experience the media more authentically, with the passion and talent of the original actors. But you don’t need to be against dubs to see what’s wrong with the ones Prime Video tested.

In videos shared by users, some of the AI-generated voice work was eerily deadpan. In one telling video Ash Lynx from Banana Fish tries to awaken a child who has been shot while speaking in a detached, dry tone. “Don’t leave me please,” he states like a robot before confronting someone without any anger in his voice. The person responds in a similarly emotionless manner.

Amazon's AI English Dub for Banana Fish is hilariously bad at times.#BANANAFISH pic.twitter.com/CtiE47W4yh

In addition to anime viewers complaining about the quality of the dubs, some expressed anger over voice actors being passed over in favor of subpar generative AI.

A viewer going by @AGESRings_on X commenting on the Banana Fish dub wrote:

[S]o many talented voice actors, and you can’t even bother to hire a couple to dub a season of a show??????????? absolutely disrespectful.

Naturally, anime voice actors took offense, too. Damian Mills, for instance, said via X that voicing a “notable queer-coded character like Kaworu” in three Evangelion movie dubs for Prime Video (in 2007, 2009, and 2012) “meant a lot, especially being queer myself.”

Mills, who also does voice acting for other anime, including One Piece (Tanaka) and Dragon Ball Super (Frieza) added, “… using AI to replace dub actors on #BananaFish? It’s insulting and I can’t support this. It’s insane to me. What’s worse is Banana Fish is an older property, so there was no urgency to get a dub created.”

Amazon also seems to have rethought its March statement announcing that it would use AI to dub content “that would not have been dubbed otherwise.” For example, in 2017, Sentai Filmworks released an English dub of No Game, No Life: Zero with human voice actors.

On Tuesday, Gizmodo reported that “several of the English language AI dubs for anime such as Banana Fish, No Game No Life: Zero, and more have now been removed.” However, some AI-generated dubs remain as of this writing, including an English dub for the anime series Pet and a Spanish one for Banana Fish, Ars Technica has confirmed.

Amazon hasn’t commented on the AI-generated dubs or why it took some of them down.

All of this comes despite Amazon’s March announcement that the AI-generated dubs would use “human expertise” for “quality control.”

The sloppy dubbing of cherished anime titles reflects a lack of precision in the broader industry as companies seek to leverage generative AI to save time and money. Prime Video has already been criticized for using AI-generated movie summaries and posters this year. And this summer, anime streaming service Crunchyroll blamed bad AI-generated subtitles on an agreement “violation” by a “third-party vendor.”"
